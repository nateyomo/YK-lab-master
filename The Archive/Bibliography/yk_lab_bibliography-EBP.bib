@article{akobengUnderstandingDiagnosticTests2007,
  title = {Understanding Diagnostic Tests 2: Likelihood Ratios, Pre- and Post-Test Probabilities and Their Use in Clinical Practice},
  shorttitle = {Understanding Diagnostic Tests 2},
  author = {Akobeng, Anthony K.},
  year = {2007},
  month = apr,
  journal = {Acta Paediatrica (Oslo, Norway: 1992)},
  volume = {96},
  number = {4},
  pages = {487--491},
  issn = {0803-5253},
  doi = {10.1111/j.1651-2227.2006.00179.x},
  abstract = {The sensitivity and specificity of a test cannot be used to estimate probability of disease in individual patients. They can, however, be combined into a single measure called the likelihood ratio which is, clinically, more useful than sensitivity or specificity. Likelihood ratios provide a summary of how many times more (or less) likely patients with a disease are to have a particular result than patients without the disease. Using the principles of the Bayes theorem, likelihood ratios can be used in conjunction with pre-test probability of disease to estimate an individual's post-test probability of disease, that is his or her chance of having disease once the result of a test is known. The Fagan's nomogram is a graphical tool which, in routine clinical practice, allows one to combine the likelihood ratio of a test with a patient's pre-test probability of disease to estimate post-test probability. CONCLUSION: Likelihood ratios summarize information about a diagnostic test by combining sensitivity and specificity. The Fagan's nomogram is a useful and convenient graphical tool that allows likelihood ratios to be used in conjunction with a patient's pre-test probability of disease to estimate the post-test probability of disease.},
  langid = {english},
  pmid = {17306009},
  keywords = {Celiac Disease,Child,Diagnostic Tests Routine,Female,Humans,Infant,Likelihood Functions,Male,Predictive Value of Tests,Probability,Risk Assessment,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/NEMEGX7G/Akobeng (2007) Understanding diagnostic tests 2.pdf}
}

@article{allamAssessmentStatisticalMethodologies2017,
  title = {Assessment of {{Statistical Methodologies}} and {{Pitfalls}} of {{Dissertations Carried Out}} at {{National Cancer Institute}}, {{Cairo University}}},
  author = {Allam, Rasha M. and Noaman, Maissa K. and Moneer, Manar M. and Elattar, Inas A.},
  year = {2017},
  month = jan,
  journal = {Asian Pacific journal of cancer prevention: APJCP},
  volume = {18},
  number = {1},
  pages = {231--237},
  issn = {2476-762X},
  doi = {10.22034/APJCP.2017.18.1.231},
  abstract = {Purpose: To identify statistical errors and pitfalls in dissertations performed as part of the requirements for the Medical Doctorate (MD) degree at the National Cancer Institute (NCI), Cairo University (CU) to improve the quality of medical research. Methods: A critical assessment of 62 MD dissertations conducted in 3 departments at NCI, CU, between 2009 and 2013 was carried out regarding statistical methodology and presentation of the results. To detect differences in study characteristics over time, grouping was into two periods; 2009-2010 and 2011-2013. Results: Statistical methods were appropriate in only 13 studies (24.5\%). The most common statistical tests applied were chi-square, log-rank, and Mann-Whitney tests. Four studies estimated sample size and/or power. Only 37.1\% and 38.7\% of dissertation results supported aims and answered the research questions, respectively. Most of results were misinterpreted (82.3\%) with misuse of statistical terminology (77.4\%). Tabular and graphical data display was independently informative in only 36 dissertations (58.1\%) with accurate titles and labels in only 17 (27.4\%). Statistical tests fulfilled the assumptions only in 29 studies; with evident misuse in 33. Ten dissertations reported non-significance regarding their primary outcome measure; the median power of the test was 35.5\% (range: 6-60\%). There was no significant change in the characteristics between the time periods. Conclusion: MD dissertations at NCI have many epidemiological and statistical defects that may compromise the external validity of the results. It is recommended to involve a biostatistician from the very start to improve study design, sample size calculation, end points estimation and measures.},
  langid = {english},
  pmcid = {PMC5563106},
  pmid = {28240524},
  keywords = {MD theses,National Cancer Institute,statistical methodology,study design},
  file = {/Users/nathanielyomogida/Zotero/storage/USFLB8N4/Allam et al. - 2017 - Assessment of Statistical Methodologies and Pitfal.pdf}
}

@article{althousePostHocPower2021,
  title = {Post {{Hoc Power}}: {{Not Empowering}}, {{Just Misleading}}},
  shorttitle = {Post {{Hoc Power}}},
  author = {Althouse, Andrew D.},
  year = {2021},
  month = mar,
  journal = {The Journal of Surgical Research},
  volume = {259},
  pages = {A3-A6},
  issn = {1095-8673},
  doi = {10.1016/j.jss.2019.10.049},
  langid = {english},
  pmid = {32814615},
  keywords = {Leadership,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/U4BYP63N/Althouse - 2021 - Post Hoc Power Not Empowering, Just Misleading.pdf}
}

@article{atogarciaTwowayMixedModel2013,
  title = {The Two-Way Mixed Model: A Long and Winding Controversy},
  shorttitle = {The Two-Way Mixed Model},
  author = {Ato Garc{\'i}a, Manuel and Vallejo Seco, Guillermo and Palmer Pol, Alfonso},
  year = {2013},
  month = feb,
  journal = {Psicothema},
  volume = {25},
  number = {1},
  pages = {130--136},
  issn = {1886-144X},
  doi = {10.7334/psicothema2012.15},
  abstract = {BACKGROUND: With the 2-way mixed model, one a fixed factor and the other random, the procedure followed to test statistical significance of the random factor has been the focus of a heated controversy in theoretical and applied statistics, and the debating continues even now. One of the main consequences of this controversy is that the position defended in the classical ANOVA texts on the hypothesis of the significance of the random effect is not the same as that defended in almost all of the professional statistical software programs. METHOD: In this paper, we deal with a detailed analysis about the controversy of mixed model and the decision about one of two basic options, the non restrictive and the restrictive model. RESULTS: Three key questions we consider to go beyond the controversy are: (1) the two classical models are equivalent, (2) the marginality principle do not allow to test main effects in presence of interactive significant effects and (3) the relevance of linear mixed approach to analyze models with fixed and random effects. CONCLUSIONS: We propose the simple solution of using the mixed linear approach with REML estimation instead of the classical linear approach, which is really unapplicable in this context.},
  langid = {english},
  pmid = {23336555},
  keywords = {Models Psychological,Models Statistical},
  file = {/Users/nathanielyomogida/Zotero/storage/CBSNXV6N/Ato García et al. - 2013 - The two-way mixed model a long and winding contro.pdf}
}

@article{bartkoIntraclassCorrelationCoefficient1966,
  title = {The {{Intraclass Correlation Coefficient}} as a {{Measure}} of {{Reliability}}},
  author = {Bartko, John J.},
  year = {1966},
  month = aug,
  journal = {Psychological Reports},
  volume = {19},
  number = {1},
  pages = {3--11},
  issn = {0033-2941, 1558-691X},
  doi = {10.2466/pr0.1966.19.1.3},
  url = {http://journals.sagepub.com/doi/10.2466/pr0.1966.19.1.3},
  urldate = {2024-02-02},
  abstract = {A procedure for estimating the reliability of sets of ratings in terms of the intraclass correlation coefficient is discussed. The procedure is based upon the analysis of variance and the estimation of variance components. For the one-way classification the intraclass correlation coefficient defined as the ratio of variances can be interpreted as a correlation coefficient. Caution, however, is urged in the application of the definition to a two-way model, i.e., one in which between-rater variance is removed. It is maintained that the frequent use of the standard definition of the one-way intraclass correlation coefficient applied to the two-way classification is not a proper procedure if in fact the coefficient is to be interpreted as a correlation coefficient. Definitions for reliability obtained from the two-way models are given which can legitimately be considered correlation coefficients.},
  langid = {english},
  file = {/Users/nathanielyomogida/Zotero/storage/SDCZ7AZ6/Bartko (1966) The Intraclass Correlation Coefficient as a Measure of Reliability.pdf}
}

@article{berndtSamplingMethods2020,
  title = {Sampling {{Methods}}},
  author = {Berndt, Andrea E.},
  year = {2020},
  month = may,
  journal = {Journal of Human Lactation: Official Journal of International Lactation Consultant Association},
  volume = {36},
  number = {2},
  pages = {224--226},
  issn = {1552-5732},
  doi = {10.1177/0890334420906850},
  abstract = {Knowledge of sampling methods is essential to design quality research. Critical questions are provided to help researchers choose a sampling method. This article reviews probability and non-probability sampling methods, lists and defines specific sampling techniques, and provides pros and cons for consideration. In addition, issues related to sampling methods are described to highlight potential problems.},
  langid = {english},
  pmid = {32155099},
  keywords = {breastfeeding,Humans,non-probability sampling,Patient Selection,probability sampling,Research Design,sampling,sampling methods,Selection Bias},
  file = {/Users/nathanielyomogida/Zotero/storage/5N6E7GSU/Berndt (2020) Sampling Methods.pdf;/Users/nathanielyomogida/Zotero/storage/IA6MVAX7/10.1177@0890334420906850.pdf.pdf}
}

@article{birkimerBackBasicsPercentage1979,
  title = {Back to Basics: {{Percentage}} Agreement Measures Are Adequate, but There Are Easier Ways},
  shorttitle = {Back to Basics},
  author = {Birkimer, J. C. and Brown, J. H.},
  year = {1979},
  journal = {Journal of Applied Behavior Analysis},
  volume = {12},
  number = {4},
  pages = {535--543},
  issn = {0021-8855},
  doi = {10.1901/jaba.1979.12-535},
  abstract = {Percentage agreement measures of interobserver agreement or "reliability" have traditionally been used to summarize observer agreement from studies using interval recording, time-sampling, and trial-scoring data collection procedures. Recent articles disagree on whether to continue using these percentage agreement measures, and on which ones to use, and what to do about chance agreements if their use is continued. Much of the disagreement derives from the need to be reasonably certain we do not accept as evidence of true interobserver agreement those agreement levels which are substantially probable as a result of chance observer agreement. The various percentage agreement measures are shown to be adequate to this task, but easier ways are discussed. Tables are given to permit checking to see if obtained disagreements are unlikely due to chance. Particularly important is the discovery of a simple rule that, when met, makes the tables unnecessary. If reliability checks using 50 or more observation occasions produce 10\% or fewer disagreements, for behavior rates from 10\% through 90\%, the agreement achieved is quite improbably the result of chance agreement.},
  langid = {english},
  pmcid = {PMC1311476},
  pmid = {16795610},
  file = {/Users/nathanielyomogida/Zotero/storage/XJSY664R/Birkimer_Brown (1979) Back to basics.pdf}
}

@article{buntingPracticalGuideAssess2019,
  title = {A {{Practical Guide}} to {{Assess}} the {{Reproducibility}} of {{Echocardiographic Measurements}}},
  author = {Bunting, Karina V. and Steeds, Richard P. and Slater, Karin and Rogers, Jennifer K. and Gkoutos, Georgios V. and Kotecha, Dipak},
  year = {2019},
  month = dec,
  journal = {Journal of the American Society of Echocardiography},
  volume = {32},
  number = {12},
  pages = {1505--1515},
  issn = {08947317},
  doi = {10.1016/j.echo.2019.08.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0894731719309460},
  urldate = {2024-03-04},
  langid = {english},
  keywords = {To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/ARH6F2IT/Bunting et al (2019) A Practical Guide to Assess the Reproducibility of Echocardiographic.pdf}
}

@article{chanLearningUnderstandingKruskalWallis1997,
  title = {Learning and Understanding the {{Kruskal-Wallis}} One-Way Analysis-of-Variance-by-Ranks Test for Differences among Three or More Independent Groups},
  author = {Chan, Y. and Walmsley, R. P.},
  year = {1997},
  month = dec,
  journal = {Physical Therapy},
  volume = {77},
  number = {12},
  pages = {1755--1762},
  issn = {0031-9023},
  doi = {10.1093/ptj/77.12.1755},
  abstract = {When several treatment methods are available for the same problem, many clinicians are faced with the task of deciding which treatment to use. Many clinicians may have conducted informal "mini-experiments" on their own to determine which treatment is best suited for the problem. These results are usually not documented or reported in a formal manner because many clinicians feel that they are "statistically challenged." Another reason may be because clinicians do not feel they have controlled enough test conditions to warrant analysis. In this update, a statistic is described that does not involve complicated statistical assumptions, making it a simple and easy-to-use statistical method. This update examines the use of two statistics and does not deal with other issues that could affect clinical research such as issues affecting credibility. For readers who want a more in-depth examination of this topic, references have been provided. The Kruskal-Wallis one-way analysis-of-variance-by-ranks test (or H test) is used to determine whether three or more independent groups are the same or different on some variable of interest when an ordinal level of data or an interval or ratio level of data is available. A hypothetical example will be presented to explain when and how to use this statistic, how to interpret results using the statistic, the advantages and disadvantages of the statistic, and what to look for in a written report. This hypothetical example will involve the use of ratio data to demonstrate how to choose between using the nonparametric H test and the more powerful parametric F test.},
  langid = {english},
  pmid = {9413454},
  keywords = {Analysis of Variance,Decision Support Techniques,Humans,Statistics Nonparametric,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/Q88B9MV6/Chan and Walmsley - 1997 - Learning and understanding the Kruskal-Wallis one-.pdf}
}

@article{chenRolesChallengesMerits2023,
  title = {The Roles, Challenges, and Merits of the p Value},
  author = {Ch{\'e}n, Oliver Y. and Bodelet, Julien S. and Saraiva, Ra{\'u}l G. and Phan, Huy and Di, Junrui and Nagels, Guy and Schwantje, Tom and Cao, Hengyi and Gou, Jiangtao and Reinen, Jenna M. and Xiong, Bin and Zhi, Bangdong and Wang, Xiaojun and {de Vos}, Maarten},
  year = {2023},
  month = dec,
  journal = {Patterns (New York, N.Y.)},
  volume = {4},
  number = {12},
  pages = {100878},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2023.100878},
  abstract = {Since the 18th century, the p value has been an important part of hypothesis-based scientific investigation. As statistical and data science engines accelerate, questions emerge: to what extent are scientific discoveries based on p values reliable and reproducible? Should one adjust the significance level or find alternatives for the p value? Inspired by these questions and everlasting attempts to address them, here, we provide a systematic examination of the p value from its roles and merits to its misuses and misinterpretations. For the latter, we summarize modest recommendations to handle them. In parallel, we present the Bayesian alternatives for seeking evidence and discuss the pooling of p values from multiple studies and datasets. Overall, we argue that the p value and hypothesis testing form a useful probabilistic decision-making mechanism, facilitating causal inference, feature selection, and predictive modeling, but that the interpretation of the p value must be contextual, considering the scientific question, experimental design, and statistical principles.},
  langid = {english},
  pmcid = {PMC10724370},
  pmid = {38106615},
  keywords = {To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/85X9STCR/Chén et al (2023) The roles, challenges, and merits of the p value.pdf}
}

@article{curran-everett-AnalysisOfRatios-NormalizedData2013,
  title = {Explorations in Statistics: The Analysis of Ratios and Normalized Data},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2013},
  month = sep,
  journal = {Advances in Physiology Education},
  volume = {37},
  number = {3},
  pages = {213--219},
  issn = {1522-1229},
  doi = {10.1152/advan.00053.2013},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This ninth installment of Explorations in Statistics explores the analysis of ratios and normalized-or standardized-data. As researchers, we compute a ratio-a numerator divided by a denominator-to compute a proportion for some biological response or to derive some standardized variable. In each situation, we want to control for differences in the denominator when the thing we really care about is the numerator. But there is peril lurking in a ratio: only if the relationship between numerator and denominator is a straight line through the origin will the ratio be meaningful. If not, the ratio will misrepresent the true relationship between numerator and denominator. In contrast, regression techniques-these include analysis of covariance-are versatile: they can accommodate an analysis of the relationship between numerator and denominator when a ratio is useless.},
  langid = {english},
  pmid = {24022766},
  keywords = {analysis of covariance,Humans,model II regression,Models Biological,Models Statistical,ordinary least-squares regression,Regression Analysis,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/QJ64HV33/Curran-Everett (2013) Explorations in statistics.pdf}
}

@article{curran-everett-AssumptionOfNormality2017,
  title = {Explorations in Statistics: The Assumption of Normality},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2017},
  month = sep,
  journal = {Advances in Physiology Education},
  volume = {41},
  number = {3},
  pages = {449--453},
  issn = {1522-1229},
  doi = {10.1152/advan.00064.2017},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This twelfth installment of Explorations in Statistics explores the assumption of normality, an assumption essential to the meaningful interpretation of a t test. Although the data themselves can be consistent with a normal distribution, they need not be. Instead, it is the theoretical distribution of the sample mean or the theoretical distribution of the difference between sample means that must be roughly normal. The most versatile approach to assess normality is to bootstrap the sample mean, the difference between sample means, or t itself. We can then assess whether the distributions of these bootstrap statistics are consistent with a normal distribution by studying their normal quantile plots. If we suspect that an inference we make from a t test may not be justified-if we suspect that the theoretical distribution of the sample mean or the theoretical distribution of the difference between sample means is not normal-then we can use a permutation method to analyze our data.},
  langid = {english},
  pmid = {28743689},
  keywords = {bootstrap,Central Limit Theorem,Data Interpretation Statistical,Models Statistical,normal quantile plot,permutation methods,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/A4IZLQIC/Curran-Everett (2017) Explorations in statistics.pdf}
}

@article{curran-everett-PermutationMethods2012,
  title = {Explorations in Statistics: Permutation Methods},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2012},
  month = sep,
  journal = {Advances in Physiology Education},
  volume = {36},
  number = {3},
  pages = {181--187},
  issn = {1522-1229},
  doi = {10.1152/advan.00072.2012},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This eighth installment of Explorations in Statistics explores permutation methods, empiric procedures we can use to assess an experimental result-to test a null hypothesis-when we are reluctant to trust statistical theory alone. Permutation methods operate on the observations-the data-we get from an experiment. A permutation procedure answers this question: out of all the possible ways we can rearrange the observations we got, in what proportion of those arrangements is the sample statistic we care about at least as extreme as the one we got? The answer to that question is the P value.},
  langid = {english},
  pmid = {22952255},
  keywords = {Data Interpretation Statistical,Empirical Research,Models Statistical,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/SEXR5CS9/Curran-Everett (2012) Explorations in statistics.pdf}
}

@article{curran-everett-StatisticalFacets-Reproducibility2016,
  title = {Explorations in Statistics: Statistical Facets of Reproducibility},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2016},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {40},
  number = {2},
  pages = {248--252},
  issn = {1522-1229},
  doi = {10.1152/advan.00042.2016},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This eleventh installment of Explorations in Statistics explores statistical facets of reproducibility. If we obtain an experimental result that is scientifically meaningful and statistically unusual, we would like to know that our result reflects a general biological phenomenon that another researcher could reproduce if (s)he repeated our experiment. But more often than not, we may learn this researcher cannot replicate our result. The National Institutes of Health and the Federation of American Societies for Experimental Biology have created training modules and outlined strategies to help improve the reproducibility of research. These particular approaches are necessary, but they are not sufficient. The principles of hypothesis testing and estimation are inherent to the notion of reproducibility in science. If we want to improve the reproducibility of our research, then we need to rethink how we apply fundamental concepts of statistics to our science.},
  langid = {english},
  pmid = {27231259},
  keywords = {Biomedical Research,estimation,Humans,hypothesis test,Models Statistical,power,Reproducibility of Results,Research Personnel,significance test,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/KEH4L4IZ/Curran-Everett (2016) Explorations in statistics.pdf}
}

@article{curran-everett-TheBootstrap2009,
  title = {Explorations in Statistics: The Bootstrap},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2009},
  month = dec,
  journal = {Advances in Physiology Education},
  volume = {33},
  number = {4},
  pages = {286--292},
  issn = {1522-1229},
  doi = {10.1152/advan.00062.2009},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This fourth installment of Explorations in Statistics explores the bootstrap. The bootstrap gives us an empirical approach to estimate the theoretical variability among possible values of a sample statistic such as the sample mean. The appeal of the bootstrap is that we can use it to make an inference about some experimental result when the statistical theory is uncertain or even unknown. We can also use the bootstrap to assess how well the statistical theory holds: that is, whether an inference we make from a hypothesis test or confidence interval is justified.},
  langid = {english},
  pmid = {19948676},
  keywords = {Humans,Models Statistical,Software,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/VV6Z4V3G/Curran-Everett (2009) Explorations in statistics.pdf}
}

@article{curran-everettAnalysisOfChange2015,
  title = {Explorations in Statistics: The Analysis of Change},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas and Williams, Calvin L.},
  year = {2015},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {39},
  number = {2},
  pages = {49--54},
  issn = {1522-1229},
  doi = {10.1152/advan.00018.2015},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This tenth installment of Explorations in Statistics explores the analysis of a potential change in some physiological response. As researchers, we often express absolute change as percent change so we can account for different initial values of the response. But this creates a problem: percent change is really just a ratio, and a ratio is infamous for its ability to mislead. This means we may fail to find a group difference that does exist, or we may find a group difference that does not exist. What kind of an approach to science is that? In contrast, analysis of covariance is versatile: it can accommodate an analysis of the relationship between absolute change and initial value when percent change is useless.},
  langid = {english},
  pmid = {26031718},
  keywords = {absolute change,analysis of covariance,Analysis of Variance,Animals,Biomedical Research,Data Interpretation Statistical,Finish reading,Humans,Least-Squares Analysis,Models Statistical,ordinary least-squares regression,percent change,Physiology,Software,symmetrized percent change,Time Factors},
  file = {/Users/nathanielyomogida/Zotero/storage/GTVSH3P9/Curran-Everett_Williams (2015) Explorations in statistics.pdf}
}

@article{curran-everettBestPracticesSeries2015,
  title = {Best {{Practices}}: A Series of Theory, Evidence, and Implementation},
  shorttitle = {Best {{Practices}}},
  author = {{Curran-Everett}, Douglas},
  year = {2015},
  month = dec,
  journal = {Advances in Physiology Education},
  volume = {39},
  number = {4},
  pages = {253},
  issn = {1522-1229},
  doi = {10.1152/advan.00099.2015},
  langid = {english},
  pmid = {26628644},
  keywords = {Humans,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/T4E4ESEX/Curran-Everett (2015) Best Practices.pdf}
}

@article{curran-everettConfidenceIntervals2009,
  title = {Explorations in Statistics: Confidence Intervals},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2009},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {33},
  number = {2},
  pages = {87--90},
  issn = {1522-1229},
  doi = {10.1152/advan.00006.2009},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This third installment of Explorations in Statistics investigates confidence intervals. A confidence interval is a range that we expect, with some level of confidence, to include the true value of a population parameter such as the mean. A confidence interval provides the same statistical information as the P value from a hypothesis test, but it circumvents the drawbacks of that hypothesis test. Even more important, a confidence interval focuses our attention on the scientific importance of some experimental result.},
  langid = {english},
  pmid = {19509392},
  keywords = {Confidence Intervals,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/AT8DEWQI/Curran-Everett (2009) Explorations in statistics.pdf}
}

@article{curran-everettCorrelation2010,
  title = {Explorations in Statistics: Correlation},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2010},
  month = dec,
  journal = {Advances in Physiology Education},
  volume = {34},
  number = {4},
  pages = {186--191},
  issn = {1522-1229},
  doi = {10.1152/advan.00068.2010},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This sixth installment of Explorations in Statistics explores correlation, a familiar technique that estimates the magnitude of a straight-line relationship between two variables. Correlation is meaningful only when the two variables are true random variables: for example, if we restrict in some way the variability of one variable, then the magnitude of the correlation will decrease. Correlation cannot help us decide if changes in one variable result in changes in the second variable, if changes in the second variable result in changes in the first variable, or if changes in a third variable result in concurrent changes in the first two variables. Correlation can help provide us with evidence that study of the nature of the relationship between x and y may be warranted in an actual experiment in which one of them is controlled.},
  langid = {english},
  pmid = {21098385},
  keywords = {Biostatistics,Computer Simulation,History 19th Century,Regression Analysis,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/RLAA33D6/Curran-Everett (2010) Explorations in statistics.pdf}
}

@article{curran-everettElucidationAndIllustration1998,
  title = {Fundamental Concepts in Statistics: Elucidation and Illustration},
  shorttitle = {Fundamental Concepts in Statistics},
  author = {{Curran-Everett}, D. and Taylor, S. and Kafadar, K.},
  year = {1998},
  month = sep,
  journal = {Journal of Applied Physiology (Bethesda, Md.: 1985)},
  volume = {85},
  number = {3},
  pages = {775--786},
  issn = {8750-7587},
  doi = {10.1152/jappl.1998.85.3.775},
  abstract = {Fundamental concepts in statistics form the cornerstone of scientific inquiry. If we fail to understand fully these fundamental concepts, then the scientific conclusions we reach are more likely to be wrong. This is more than supposition: for 60 years, statisticians have warned that the scientific literature harbors misunderstandings about basic statistical concepts. Original articles published in 1996 by the American Physiological Society's journals fared no better in their handling of basic statistical concepts. In this review, we summarize the two main scientific uses of statistics: hypothesis testing and estimation. Most scientists use statistics solely for hypothesis testing; often, however, estimation is more useful. We also illustrate the concepts of variability and uncertainty, and we demonstrate the essential distinction between statistical significance and scientific importance. An understanding of concepts such as variability, uncertainty, and significance is necessary, but it is not sufficient; we show also that the numerical results of statistical analyses have limitations.},
  langid = {english},
  pmid = {9729547},
  keywords = {Humans,Physiology,Population,Research Design,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/6AMPBCGA/Curran-Everett et al (1998) Fundamental concepts in statistics.pdf}
}

@article{curran-everettEvolutionStatisticsValues2020,
  title = {Evolution in Statistics: {{P}} Values, Statistical Significance, Kayaks, and Walking Trees},
  shorttitle = {Evolution in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2020},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {44},
  number = {2},
  pages = {221--224},
  issn = {1522-1229},
  doi = {10.1152/advan.00054.2020},
  langid = {english},
  pmid = {32412384},
  keywords = {Animals,Biomedical Research,Data Accuracy,Data Interpretation Statistical,Guidelines as Topic,Humans,Models Statistical,Periodicals as Topic,Research Design,Terminology as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/G663A4JS/Curran-Everett (2020) Evolution in statistics.pdf}
}

@article{curran-everettHypothesisTestingPvalue2009,
  title = {Explorations in Statistics: Hypothesis Tests and {{P}} Values},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2009},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {33},
  number = {2},
  pages = {81--86},
  issn = {1522-1229},
  doi = {10.1152/advan.90218.2008},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This second installment of Explorations in Statistics delves into test statistics and P values, two concepts fundamental to the test of a scientific null hypothesis. The essence of a test statistic is that it compares what we observe in the experiment to what we expect to see if the null hypothesis is true. The P value associated with the magnitude of that test statistic answers this question: if the null hypothesis is true, what proportion of possible values of the test statistic are at least as extreme as the one I got? Although statisticians continue to stress the limitations of hypothesis tests, there are two realities we must acknowledge: hypothesis tests are ingrained within science, and the simple test of a null hypothesis can be useful. As a result, it behooves us to explore the notions of hypothesis tests, test statistics, and P values.},
  langid = {english},
  pmid = {19509391},
  keywords = {Algorithms,Research Design,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/C4V4LH9W/Curran-Everett (2009) Explorations in statistics.pdf}
}

@article{curran-everettLogTransformation2018,
  title = {Explorations in Statistics: The Log Transformation},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2018},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {42},
  number = {2},
  pages = {343--347},
  issn = {1522-1229},
  doi = {10.1152/advan.00018.2018},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This thirteenth installment of Explorations in Statistics explores the log transformation, an established technique that rescales the actual observations from an experiment so that the assumptions of some statistical analysis are better met. A general assumption in statistics is that the variability of some response Y is homogeneous across groups or across some predictor variable X. If the variability-the standard deviation-varies in rough proportion to the mean value of Y, a log transformation can equalize the standard deviations. Moreover, if the actual observations from an experiment conform to a skewed distribution, then a log transformation can make the theoretical distribution of the sample mean more consistent with a normal distribution. This is important: the results of a one-sample t test are meaningful only if the theoretical distribution of the sample mean is roughly normal. If we log-transform our observations, then we want to confirm the transformation was useful. We can do this if we use the Box-Cox method, if we bootstrap the sample mean and the statistic t itself, and if we assess the residual plots from the statistical model of the actual and transformed sample observations.},
  langid = {english},
  pmid = {29761718},
  keywords = {bootstrap,Central Limit Theorem,Data Collection,Humans,Linear Models,Models Statistical,normal quantile plot,Physiology,residual plots,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/8GUL46X7/Curran-Everett (2018) Explorations in statistics.pdf}
}

@article{curran-everettMultipleComparisonsPhilosophies2000,
  title = {Multiple Comparisons: Philosophies and Illustrations},
  shorttitle = {Multiple Comparisons},
  author = {{Curran-Everett}, D.},
  year = {2000},
  month = jul,
  journal = {American Journal of Physiology. Regulatory, Integrative and Comparative Physiology},
  volume = {279},
  number = {1},
  pages = {R1-8},
  issn = {0363-6119},
  doi = {10.1152/ajpregu.2000.279.1.R1},
  abstract = {Statistical procedures underpin the process of scientific discovery. As researchers, one way we use these procedures is to test the validity of a null hypothesis. Often, we test the validity of more than one null hypothesis. If we fail to use an appropriate procedure to account for this multiplicity, then we are more likely to reach a wrong scientific conclusion-we are more likely to make a mistake. In physiology, experiments that involve multiple comparisons are common: of the original articles published in 1997 by the American Physiological Society, approximately 40\% cite a multiple comparison procedure. In this review, I demonstrate the statistical issue embedded in multiple comparisons, and I summarize the philosophies of handling this issue. I also illustrate the three procedures-Newman-Keuls, Bonferroni, least significant difference-cited most often in my literature review; each of these procedures is of limited practical value. Last, I demonstrate the false discovery rate procedure, a promising development in multiple comparisons. The false discovery rate procedure may be the best practical solution to the problems of multiple comparisons that exist within physiology and other scientific disciplines.},
  langid = {english},
  pmid = {10896857},
  keywords = {Animals,Models Statistical,Predictive Value of Tests,Rats,Reproducibility of Results,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/R5IGP8SF/Curran-Everett (2000) Multiple comparisons.pdf}
}

@article{curran-everettPower2010,
  title = {Explorations in Statistics: Power},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2010},
  month = jun,
  journal = {Advances in Physiology Education},
  volume = {34},
  number = {2},
  pages = {41--43},
  issn = {1522-1229},
  doi = {10.1152/advan.00001.2010},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This fifth installment of Explorations in Statistics revisits power, a concept fundamental to the test of a null hypothesis. Power is the probability that we reject the null hypothesis when it is false. Four things affect power: the probability with which we are willing to reject-by mistake-a true null hypothesis, the magnitude of the difference we want to be able to detect, the variability of the underlying population, and the number of observations in our sample. In an application to an Institutional Animal Care and Use Committee or to the National Institutes of Health, we define power to justify the sample size we propose.},
  langid = {english},
  pmid = {20522895},
  keywords = {Biostatistics,Computer Simulation,Data Interpretation Statistical,Probability,Sample Size,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/JPKSR6A6/Curran-Everett (2010) Explorations in statistics.pdf}
}

@article{curran-everettRegression2011,
  title = {Explorations in Statistics: Regression},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2011},
  month = dec,
  journal = {Advances in Physiology Education},
  volume = {35},
  number = {4},
  pages = {347--352},
  issn = {1522-1229},
  doi = {10.1152/advan.00051.2011},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This seventh installment of Explorations in Statistics explores regression, a technique that estimates the nature of the relationship between two things for which we may only surmise a mechanistic or predictive connection. Regression helps us answer three questions: does some variable Y depend on another variable X; if so, what is the nature of the relationship between Y and X; and for some value of X, what value of Y do we predict? Residual plots are an essential component of a thorough regression analysis: they help us decide if our statistical regression model of the relationship between Y and X is appropriate.},
  langid = {english},
  pmid = {22139769},
  keywords = {Animals,Biomedical Research,Data Interpretation Statistical,Humans,Models Statistical,Physiology,Regression Analysis,Software,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/PB5W9ZTU/Curran-Everett (2011) Explorations in statistics.pdf}
}

@article{curran-everettStandardDeviationaStandardError2008,
  title = {Explorations in Statistics: Standard Deviations and Standard Errors},
  shorttitle = {Explorations in Statistics},
  author = {{Curran-Everett}, Douglas},
  year = {2008},
  month = sep,
  journal = {Advances in Physiology Education},
  volume = {32},
  number = {3},
  pages = {203--208},
  issn = {1522-1229},
  doi = {10.1152/advan.90123.2008},
  abstract = {Learning about statistics is a lot like learning about science: the learning is more meaningful if you can actively explore. This series in Advances in Physiology Education provides an opportunity to do just that: we will investigate basic concepts in statistics using the free software package R. Because this series uses R solely as a vehicle with which to explore basic concepts in statistics, I provide the requisite R commands. In this inaugural paper we explore the essential distinction between standard deviation and standard error: a standard deviation estimates the variability among sample observations whereas a standard error of the mean estimates the variability among theoretical sample means. If we fail to report the standard deviation, then we fail to fully report our data. Because it incorporates information about sample size, the standard error of the mean is a misguided estimate of variability among observations. Instead, the standard error of the mean provides an estimate of the uncertainty of the true value of the population mean.},
  langid = {english},
  pmid = {18794241},
  keywords = {Confidence Intervals,Data Interpretation Statistical,Physiology,Probability,Software,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/6Z65I25C/Curran-Everett (2008) Explorations in statistics.pdf}
}

@article{duttaRanksumTestClustered2016,
  title = {A Rank-Sum Test for Clustered Data When the Number of Subjects in a Group within a Cluster Is Informative},
  author = {Dutta, Sandipan and Datta, Somnath},
  year = {2016},
  month = jun,
  journal = {Biometrics},
  volume = {72},
  number = {2},
  pages = {432--440},
  issn = {1541-0420},
  doi = {10.1111/biom.12447},
  abstract = {The Wilcoxon rank-sum test is a popular nonparametric test for comparing two independent populations (groups). In recent years, there have been renewed attempts in extending the Wilcoxon rank sum test for clustered data, one of which (Datta and Satten, 2005, Journal of the American Statistical Association 100, 908-915) addresses the issue of informative cluster size, i.e., when the outcomes and the cluster size are correlated. We are faced with a situation where the group specific marginal distribution in a cluster depends on the number of observations in that group (i.e., the intra-cluster group size). We develop a novel extension of the rank-sum test for handling this situation. We compare the performance of our test with the Datta-Satten test, as well as the naive Wilcoxon rank sum test. Using a naturally occurring simulation model of informative intra-cluster group size, we show that only our test maintains the correct size. We also compare our test with a classical signed rank test based on averages of the outcome values in each group paired by the cluster membership. While this test maintains the size, it has lower power than our test. Extensions to multiple group comparisons and the case of clusters not having samples from all groups are also discussed. We apply our test to determine whether there are differences in the attachment loss between the upper and lower teeth and between mesial and buccal sites of periodontal patients.},
  langid = {english},
  pmcid = {PMC4870168},
  pmid = {26575695},
  keywords = {Biometry,Cluster Analysis,Computer Simulation,Correlated data,Data Interpretation Statistical,Dental data,Humans,Nonparametric tests,Periodontal Attachment Loss,Sample Size,Statistics Nonparametric,Tooth,Wilcoxon rank-sum test,Within-cluster resampling},
  file = {/Users/nathanielyomogida/Zotero/storage/BKN5LV3L/dutta2015.pdf.pdf;/Users/nathanielyomogida/Zotero/storage/G9WPXZRJ/Dutta and Datta - 2016 - A rank-sum test for clustered data when the number.pdf}
}

@article{eusebiDiagnosticAccuracyMeasures2013,
  title = {Diagnostic Accuracy Measures},
  author = {Eusebi, Paolo},
  year = {2013},
  journal = {Cerebrovascular Diseases (Basel, Switzerland)},
  volume = {36},
  number = {4},
  pages = {267--272},
  issn = {1421-9786},
  doi = {10.1159/000353863},
  abstract = {BACKGROUND: An increasing number of diagnostic tests and biomarkers have been validated during the last decades, and this will still be a prominent field of research in the future because of the need for personalized medicine. Strict evaluation is needed whenever we aim at validating any potential diagnostic tool, and the first requirement a new testing procedure must fulfill is diagnostic accuracy. SUMMARY: Diagnostic accuracy measures tell us about the ability of a test to discriminate between and/or predict disease and health. This discriminative and predictive potential can be quantified by measures of diagnostic accuracy such as sensitivity and specificity, predictive values, likelihood ratios, area under the receiver operating characteristic curve, overall accuracy and diagnostic odds ratio. Some measures are useful for discriminative purposes, while others serve as a predictive tool. Measures of diagnostic accuracy vary in the way they depend on the prevalence, spectrum and definition of the disease. In general, measures of diagnostic accuracy are extremely sensitive to the design of the study. Studies not meeting strict methodological standards usually over- or underestimate the indicators of test performance and limit the applicability of the results of the study. KEY MESSAGES: The testing procedure should be verified on a reasonable population, including people with mild and severe disease, thus providing a comparable spectrum. Sensitivities and specificities are not predictive measures. Predictive values depend on disease prevalence, and their conclusions can be transposed to other settings only for studies which are based on a suitable population (e.g. screening studies). Likelihood ratios should be an optimal choice for reporting diagnostic accuracy. Diagnostic accuracy measures must be reported with their confidence intervals. We always have to report paired measures (sensitivity and specificity, predictive values or likelihood ratios) for clinically meaningful thresholds. How much discriminative or predictive power we need depends on the clinical diagnostic pathway and on misclassification (false positives/negatives) costs.},
  langid = {english},
  pmid = {24135733},
  keywords = {Area Under Curve,Diagnostic Tests Routine,Discriminant Analysis,Humans,Likelihood Functions,Models Statistical,Odds Ratio,Predictive Value of Tests,Prognosis,Reference Standards,Reference Values,Reproducibility of Results,ROC Curve,Severity of Illness Index},
  file = {/Users/nathanielyomogida/Zotero/storage/SF3NJZEY/Eusebi (2013) Diagnostic accuracy measures.pdf}
}

@article{fletcherEvidencebasedApproachMedical1997,
  title = {Evidence-Based Approach to the Medical Literature},
  author = {Fletcher, R. H. and Fletcher, S. W.},
  year = {1997},
  month = apr,
  journal = {Journal of General Internal Medicine},
  volume = {12 Suppl 2},
  number = {Suppl 2},
  pages = {S5-14},
  issn = {0884-8734},
  doi = {10.1046/j.1525-1497.12.s2.1.x},
  langid = {english},
  pmcid = {PMC1497222},
  pmid = {9127238},
  keywords = {Computer Communication Networks,Education Medical Continuing,Evidence-Based Medicine,Guidelines as Topic,Humans,Information Services,Information Storage and Retrieval,Information Systems,Journalism Medical,Medical Informatics Applications,Medical Informatics Computing,MEDLINE,Physician-Patient Relations,United States},
  file = {/Users/nathanielyomogida/Zotero/storage/RMZBMN4V/Fletcher_Fletcher (1997) Evidence-based approach to the medical literature.pdf}
}

@article{franceschiniMinimalClinicallyImportant2023,
  title = {The {{Minimal Clinically Important Difference Changes Greatly Based}} on the {{Different Calculation Methods}}},
  author = {Franceschini, Marco and Boffa, Angelo and Pignotti, Elettra and Andriolo, Luca and Zaffagnini, Stefano and Filardo, Giuseppe},
  year = {2023},
  month = mar,
  journal = {The American Journal of Sports Medicine},
  volume = {51},
  number = {4},
  pages = {1067--1073},
  issn = {1552-3365},
  doi = {10.1177/03635465231152484},
  abstract = {BACKGROUND: The minimal clinically important difference (MCID) for patient-reported outcome measures (PROMs) expresses both the extent of the improvement and the value that patients place on it. MCID use is becoming increasingly widespread to understand the clinical efficacy of a given treatment, define guidelines for clinical practice, and properly interpret trial results. However, there is still large heterogeneity in the different calculation methods. PURPOSE: To calculate and compare the MCID threshold values of a PROM by applying various methods and analyzing their effect on the study results interpretation. STUDY DESIGN: Cohort study (Diagnosis); Level of evidence, 3. METHODS: The data set used to investigate the different MCID calculation approaches was based on a database of 312 patients affected by knee osteoarthritis and treated with intra-articular platelet-rich plasma. MCID values were calculated on the International Knee Documentation Committee (IKDC) subjective score at 6 months using 2 approaches: 9 methodologies referred to an anchor-based approach and 8 methodologies to a distribution-based approach. The obtained threshold values were applied to the same series of patients to understand the effect of using different MCID methods in evaluating patient response to treatment. RESULTS: The different methods employed led to MCID values ranging from 1.8 to 25.9 points. The anchor-based methods ranged from 6.3 to 25.9, while the distribution-based ones were from 1.8 to 13.8 points, showing a 4.1{\texttimes} variation of the MCID values within the anchor-based methods and a 7.6{\texttimes} variation within the distribution-based methods. The percentage of patients who reached the MCID for the IKDC subjective score changed based on the specific calculation method used. Among the anchor-based methods, this value varied from 24.0\% to 66.0\%, while among the distribution-based methods, the percentage of patients reaching the MCID varied from 44.6\% to 75.9\%. CONCLUSION: This study proved that different MCID calculation methods lead to highly heterogeneous values, which significantly affect the percentage of patients achieving the MCID in a given population. The wide-ranging thresholds obtained with the different methodologies make it difficult to evaluate the real effectiveness of a given treatment questioning the usefulness of MCID, as currently available, in the clinical research.},
  langid = {english},
  pmcid = {PMC10026158},
  pmid = {36811558},
  keywords = {Cohort Studies,Humans,International Knee Documentation Committee (IKDC),knee,Knee Joint,Minimal Clinically Important Difference,minimal clinically important difference (MCID),osteoarthritis,Osteoarthritis Knee,patient-reported outcome measure (PROM),platelet-rich plasma (PRP),Treatment Outcome},
  file = {/Users/nathanielyomogida/Zotero/storage/DEN4NLLB/Franceschini et al (2023) The Minimal Clinically Important Difference Changes Greatly Based on the.pdf}
}

@article{gallisRelativeMeasuresAssociation2019,
  title = {Relative {{Measures}} of {{Association}} for {{Binary Outcomes}}: {{Challenges}} and {{Recommendations}} for the {{Global Health Researcher}}},
  shorttitle = {Relative {{Measures}} of {{Association}} for {{Binary Outcomes}}},
  author = {Gallis, John A. and Turner, Elizabeth L.},
  year = {2019},
  month = nov,
  journal = {Annals of Global Health},
  volume = {85},
  number = {1},
  pages = {137},
  issn = {2214-9996},
  doi = {10.5334/aogh.2581},
  abstract = {BACKGROUND: Binary outcomes-which have two distinct levels (e.g., disease yes/no)-are commonly collected in global health research. The relative association of an exposure (e.g., a treatment) and such an outcome can be quantified using a ratio measure such as a risk ratio or an odds ratio. Although the odds ratio is more frequently reported than the risk ratio, many researchers, policymakers, and the general public frequently interpret it as a risk ratio. This is particularly problematic when the outcome is common because the magnitude of association is larger on the odds ratio scale than the risk ratio scale. Some recently published global health studies included misinterpretation of the odds ratio, which we hypothesize is because statistical methods for risk ratio estimation are not well known in the global health research community. OBJECTIVES: To compare and contrast available statistical methods to estimate relative measures of association for binary outcomes and to provide recommendations regarding their use. METHODS: Logistic regression for odds ratios and four approaches for risk ratios: two direct regression approaches (modified log-Poisson and log-binomial) and two indirect methods (standardization and substitution) based on logistic regression. FINDINGS: Illustrative examples demonstrate that misinterpretation of the odds ratio remains a common issue in global health research. Among the four methods presented for estimation of risk ratios, the modified log-Poisson approach is generally preferred because it has the best numerical performance and it is as easy to implement as is logistic regression for odds ratio estimation. CONCLUSIONS: We conclude that, when study design allows, studies with binary outcomes should preferably report risk ratios to measure relative association.},
  langid = {english},
  pmcid = {PMC6873895},
  pmid = {31807416},
  keywords = {Biomedical Research,Data Interpretation Statistical,Global Health,Humans,Logistic Models,Odds Ratio,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/LEJHERW2/Gallis_Turner (2019) Relative Measures of Association for Binary Outcomes.pdf}
}

@article{garofaloInteractionEffectAre2022,
  title = {Interaction Effect: {{Are}} You Doing the Right Thing?},
  shorttitle = {Interaction Effect},
  author = {Garofalo, Sara and Giovagnoli, Sara and Orsoni, Matteo and Starita, Francesca and Benassi, Mariagrazia},
  year = {2022},
  journal = {PloS One},
  volume = {17},
  number = {7},
  pages = {e0271668},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0271668},
  abstract = {How to correctly interpret interaction effects has been largely discussed in scientific literature. Nevertheless, misinterpretations are still frequently observed, and neuroscience is not exempt from this trend. We reviewed 645 papers published from 2019 to 2020 and found that, in the 93.2\% of studies reporting a statistically significant interaction effect (N = 221), post-hoc pairwise comparisons were the designated method adopted to interpret its results. Given the widespread use of this approach, we aim to: (1) highlight its limitations and how it can lead to misinterpretations of the interaction effect; (2) discuss more effective and powerful ways to correctly interpret interaction effects, including both explorative and model selection procedures. The paper provides practical examples and freely accessible online materials to reproduce all analyses.},
  langid = {english},
  pmcid = {PMC9299307},
  pmid = {35857797},
  keywords = {Finish reading,Research Design},
  file = {/Users/nathanielyomogida/Zotero/storage/CSFQQ4T2/Garofalo et al (2022) Interaction effect.pdf}
}

@article{georgeWhatsRiskDifferentiating2020,
  title = {What's the {{Risk}}: {{Differentiating Risk Ratios}}, {{Odds Ratios}}, and {{Hazard Ratios}}?},
  shorttitle = {What's the {{Risk}}},
  author = {George, Andrew and Stead, Thor S. and Ganti, Latha},
  year = {2020},
  month = aug,
  journal = {Cureus},
  volume = {12},
  number = {8},
  pages = {e10047},
  issn = {2168-8184},
  doi = {10.7759/cureus.10047},
  abstract = {Risk ratios, odds ratios, and hazard ratios are three common, but often misused, statistical measures in clinical research. In this paper, the authors dissect what each of these terms define, and provide examples from the medical literature to illustrate each of these statistical measures. Finally, the correct and incorrect methods to use these measures are summarized.},
  langid = {english},
  pmcid = {PMC7515812},
  pmid = {32983737},
  keywords = {hazard ratio,Odds Ratio,risk ratio,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/HNKQJRW9/George et al (2020) What's the Risk.pdf}
}

@article{giavarinaUnderstandingBlandAltman2015,
  title = {Understanding {{Bland Altman}} Analysis},
  author = {Giavarina, Davide},
  year = {2015},
  journal = {Biochemia Medica},
  volume = {25},
  number = {2},
  pages = {141--151},
  issn = {1330-0962},
  doi = {10.11613/BM.2015.015},
  abstract = {In a contemporary clinical laboratory it is very common to have to assess the agreement between two quantitative methods of measurement. The correct statistical approach to assess this degree of agreement is not obvious. Correlation and regression studies are frequently proposed. However, correlation studies the relationship between one variable and another, not the differences, and it is not recommended as a method for assessing the comparability between methods. In 1983 Altman and Bland (B\&A) proposed an alternative analysis, based on the quantification of the agreement between two quantitative measurements by studying the mean difference and constructing limits of agreement. The B\&A plot analysis is a simple way to evaluate a bias between the mean differences, and to estimate an agreement interval, within which 95\% of the differences of the second method, compared to the first one, fall. Data can be analyzed both as unit differences plot and as percentage differences plot. The B\&A plot method only defines the intervals of agreements, it does not say whether those limits are acceptable or not. Acceptable limits must be defined a priori, based on clinical necessity, biological considerations or other goals. The aim of this article is to provide guidance on the use and interpretation of Bland Altman analysis in method comparison studies.},
  langid = {english},
  pmcid = {PMC4470095},
  pmid = {26110027},
  keywords = {agreement analysis,Bland-Altman,Clinical Laboratory Techniques,correlation of data,Finish reading,laboratory research,method comparison,Models Theoretical,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/MJ8MTWUM/Giavarina (2015) Understanding Bland Altman analysis.pdf}
}

@article{gisevInterraterAgreementInterrater2013,
  title = {Interrater Agreement and Interrater Reliability: Key Concepts, Approaches, and Applications},
  shorttitle = {Interrater Agreement and Interrater Reliability},
  author = {Gisev, Natasa and Bell, J. Simon and Chen, Timothy F.},
  year = {2013},
  journal = {Research in social \& administrative pharmacy: RSAP},
  volume = {9},
  number = {3},
  pages = {330--338},
  issn = {1934-8150},
  doi = {10.1016/j.sapharm.2012.04.004},
  abstract = {Evaluations of interrater agreement and interrater reliability can be applied to a number of different contexts and are frequently encountered in social and administrative pharmacy research. The objectives of this study were to highlight key differences between interrater agreement and interrater reliability; describe the key concepts and approaches to evaluating interrater agreement and interrater reliability; and provide examples of their applications to research in the field of social and administrative pharmacy. This is a descriptive review of interrater agreement and interrater reliability indices. It outlines the practical applications and interpretation of these indices in social and administrative pharmacy research. Interrater agreement indices assess the extent to which the responses of 2 or more independent raters are concordant. Interrater reliability indices assess the extent to which raters consistently distinguish between different responses. A number of indices exist, and some common examples include Kappa, the Kendall coefficient of concordance, Bland-Altman plots, and the intraclass correlation coefficient. Guidance on the selection of an appropriate index is provided. In conclusion, selection of an appropriate index to evaluate interrater agreement or interrater reliability is dependent on a number of factors including the context in which the study is being undertaken, the type of variable under consideration, and the number of raters making assessments.},
  langid = {english},
  pmid = {22695215},
  keywords = {Data Interpretation Statistical,Finish reading,Humans,Observer Variation,Pharmacy Administration,Reproducibility of Results,Research Design},
  file = {/Users/nathanielyomogida/Zotero/storage/EDZEQ7C6/Gisev et al (2013) Interrater agreement and interrater reliability.pdf}
}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english},
  pmcid = {PMC4877414},
  pmid = {27209009},
  keywords = {Confidence intervals,Confidence Intervals,Data Interpretation Statistical,Humans,Hypothesis testing,Null testing,P value,Power,Probability,Significance tests,Statistical testing,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/HUDZQMN8/greenland2016.pdf.pdf;/Users/nathanielyomogida/Zotero/storage/RCV8CGVI/Greenland et al (2016) Statistical tests, P values, confidence intervals, and power.pdf}
}

@article{harrisonQualityAssessmentDiverse2021,
  title = {Quality Assessment with Diverse Studies ({{QuADS}}): An Appraisal Tool for Methodological and Reporting Quality in Systematic Reviews of Mixed- or Multi-Method Studies},
  shorttitle = {Quality Assessment with Diverse Studies ({{QuADS}})},
  author = {Harrison, Reema and Jones, Benjamin and Gardner, Peter and Lawton, Rebecca},
  year = {2021},
  month = feb,
  journal = {BMC health services research},
  volume = {21},
  number = {1},
  pages = {144},
  issn = {1472-6963},
  doi = {10.1186/s12913-021-06122-y},
  abstract = {BACKGROUND: In the context of the volume of mixed- and multi-methods studies in health services research, the present study sought to develop an appraisal tool to determine the methodological and reporting quality of such studies when included in systematic reviews. Evaluative evidence regarding the design and use of our existing Quality Assessment Tool for Studies with Diverse Designs (QATSDD) was synthesised to enhance and refine it for application across health services research. METHODS: Secondary data were collected through a literature review of all articles identified using Google Scholar that had cited the QATSDD tool from its inception in 2012 to December 2019. First authors of all papers that had cited the QATSDD (n=197) were also invited to provide further evaluative data via a qualitative online survey. Evaluative findings from the survey and literature review were synthesised narratively and these data used to identify areas requiring refinement. The refined tool was subject to inter-rater reliability, face and content validity analyses. RESULTS: Key limitations of the QATSDD tool identified related to a lack of clarity regarding scope of use of the tool and in the ease of application of criteria beyond experimental psychological research. The Quality Appraisal for Diverse Studies (QuADS) tool emerged as a revised tool to address the limitations of the QATSDD. The QuADS tool demonstrated substantial inter-rater reliability (k=0.66), face and content validity for application in systematic reviews with mixed, or multi-methods health services research. CONCLUSION: Our findings highlight the perceived value of appraisal tools to determine the methodological and reporting quality of studies in reviews that include heterogeneous studies. The QuADS tool demonstrates strong reliability and ease of use for application to multi or mixed-methods health services research.},
  langid = {english},
  pmcid = {PMC7885606},
  pmid = {33588842},
  keywords = {Adolescent,Aged,Aged 80 and over,Child,Health services research,Humans,Meta-Analysis as Topic,Mixed-methods research,Multi-methods research,Quality appraisal,Quality Control,Reproducibility of Results,Research Design,Systematic review,Systematic Reviews as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/J5JU2C7Q/Harrison et al (2021) Quality assessment with diverse studies (QuADS).pdf}
}

@article{holmesPrincipleInverseEffectiveness2009,
  title = {The Principle of Inverse Effectiveness in Multisensory Integration: Some Statistical Considerations},
  shorttitle = {The Principle of Inverse Effectiveness in Multisensory Integration},
  author = {Holmes, Nicholas P.},
  year = {2009},
  month = may,
  journal = {Brain Topography},
  volume = {21},
  number = {3-4},
  pages = {168--176},
  issn = {1573-6792},
  doi = {10.1007/s10548-009-0097-2},
  abstract = {The principle of inverse effectiveness (PoIE) in multisensory integration states that, as the responsiveness to individual sensory stimuli decreases, the strength of multisensory integration increases. I discuss three potential problems in the analysis of multisensory data with regard to the PoIE. First, due to 'regression towards the mean,' the PoIE may often be observed in datasets that are analysed post-hoc (i.e., when sorting the data by the unisensory responses). The solution is to design discrete levels of stimulus intensity a priori. Second, due to neurophysiological or methodological constraints on responsiveness, the PoIE may be, in part, a consequence of 'floor' and 'ceiling' effects. The solution is to avoid analysing or interpreting data that are too close to the limits of responsiveness, enabling both enhancement and suppression to be reliably observed. Third, the choice of units of measurement may affect whether the PoIE is observed in a given dataset. Both relative (\%) and absolute (raw) measurements have advantages, but the interpretation of both is affected by systematic changes in response variability with changes in response mean, an issue that may be addressed by using measures of discriminability or effect-size such as Cohen's d. Most importantly, randomising or permuting a dataset to construct a null distribution of a test parameter may best indicate whether any observed inverse effectiveness specifically characterises multisensory integration. When these considerations are taken into account, the PoIE may disappear or even reverse in a given dataset. I conclude that caution should be exercised when interpreting data that appear to follow the PoIE.},
  langid = {english},
  pmid = {19404728},
  keywords = {Action Potentials,Animals,Auditory Perception,Brain,Computer Simulation,Data Interpretation Statistical,Excitatory Postsynaptic Potentials,Humans,Models Neurological,Neurons,Signal Processing Computer-Assisted,Superior Colliculi,Synaptic Transmission,To Read,Touch Perception,Visual Perception},
  file = {/Users/nathanielyomogida/Zotero/storage/8U249IMK/Holmes (2009) The principle of inverse effectiveness in multisensory integration.pdf}
}

@article{kooGuidelineSelectingReporting2016,
  title = {A {{Guideline}} of {{Selecting}} and {{Reporting Intraclass Correlation Coefficients}} for {{Reliability Research}}},
  author = {Koo, Terry K. and Li, Mae Y.},
  year = {2016},
  month = jun,
  journal = {Journal of Chiropractic Medicine},
  volume = {15},
  number = {2},
  pages = {155--163},
  issn = {1556-3707},
  doi = {10.1016/j.jcm.2016.02.012},
  abstract = {OBJECTIVE: Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis. DISCUSSION FOR RESEARCHERS: There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, "model," "type," and "definition" selections. DISCUSSION FOR READERS: When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95\% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively. CONCLUSION: This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.},
  langid = {english},
  pmcid = {PMC4913118},
  pmid = {27330520},
  keywords = {Finish reading,Reliability and validity,Research,Statistics},
  file = {/Users/nathanielyomogida/Zotero/storage/43PHZTQP/Koo_Li (2016) A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for.pdf}
}

@article{leeStandardDeviationStandard2015,
  title = {Standard Deviation and Standard Error of the Mean},
  author = {Lee, Dong Kyu and In, Junyong and Lee, Sangseok},
  year = {2015},
  month = jun,
  journal = {Korean Journal of Anesthesiology},
  volume = {68},
  number = {3},
  pages = {220--223},
  issn = {2005-6419},
  doi = {10.4097/kjae.2015.68.3.220},
  abstract = {In most clinical and experimental studies, the standard deviation (SD) and the estimated standard error of the mean (SEM) are used to present the characteristics of sample data and to explain statistical analysis results. However, some authors occasionally muddle the distinctive usage between the SD and SEM in medical literature. Because the process of calculating the SD and SEM includes different statistical inferences, each of them has its own meaning. SD is the dispersion of data in a normal distribution. In other words, SD indicates how accurately the mean represents sample data. However the meaning of SEM includes statistical inference based on the sampling distribution. SEM is the SD of the theoretical distribution of the sample means (the sampling distribution). While either SD or SEM can be applied to describe data and statistical results, one should be aware of reasonable methods with which to use SD and SEM. We aim to elucidate the distinctions between SD and SEM and to provide proper usage guidelines for both, which summarize data and describe statistical results.},
  langid = {english},
  pmcid = {PMC4452664},
  pmid = {26045923},
  keywords = {Standard deviation,Standard error of the mean,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/3MS92PMQ/Lee et al (2015) Standard deviation and standard error of the mean.pdf}
}

@article{liljequistIntraclassCorrelationDiscussion2019,
  title = {Intraclass Correlation - {{A}} Discussion and Demonstration of Basic Features},
  author = {Liljequist, David and Elfving, Britt and Skavberg Roaldsen, Kirsti},
  year = {2019},
  journal = {PloS One},
  volume = {14},
  number = {7},
  pages = {e0219854},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0219854},
  abstract = {A re-analysis of intraclass correlation (ICC) theory is presented together with Monte Carlo simulations of ICC probability distributions. A partly revised and simplified theory of the single-score ICC is obtained, together with an alternative and simple recipe for its use in reliability studies. Our main, practical conclusion is that in the analysis of a reliability study it is neither necessary nor convenient to start from an initial choice of a specified statistical model. Rather, one may impartially use all three single-score ICC formulas. A near equality of the three ICC values indicates the absence of bias (systematic error), in which case the classical (one-way random) ICC may be used. A consistency ICC larger than absolute agreement ICC indicates the presence of non-negligible bias; if so, classical ICC is invalid and misleading. An F-test may be used to confirm whether biases are present. From the resulting model (without or with bias) variances and confidence intervals may then be calculated. In presence of bias, both absolute agreement ICC and consistency ICC should be reported, since they give different and complementary information about the reliability of the method. A clinical example with data from the literature is given.},
  langid = {english},
  pmcid = {PMC6645485},
  pmid = {31329615},
  keywords = {Correlation of Data,Data Interpretation Statistical,Electromyography,Humans,Monte Carlo Method,Software,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/24YTZRK9/Liljequist et al. - 2019 - Intraclass correlation - A discussion and demonstr.pdf}
}

@article{liWilcoxonSignedrankStatistic2014,
  title = {Wilcoxon's Signed-Rank Statistic: What Null Hypothesis and Why It Matters},
  shorttitle = {Wilcoxon's Signed-Rank Statistic},
  author = {Li, Heng and Johnson, Terri},
  year = {2014},
  journal = {Pharmaceutical Statistics},
  volume = {13},
  number = {5},
  pages = {281--285},
  issn = {1539-1612},
  doi = {10.1002/pst.1628},
  abstract = {In statistical literature, the term 'signed-rank test' (or 'Wilcoxon signed-rank test') has been used to refer to two distinct tests: a test for symmetry of distribution and a test for the median of a symmetric distribution, sharing a common test statistic. To avoid potential ambiguity, we propose to refer to those two tests by different names, as 'test for symmetry based on signed-rank statistic' and 'test for median based on signed-rank statistic', respectively. The utility of such terminological differentiation should become evident through our discussion of how those tests connect and contrast with sign test and one-sample t-test. Published 2014. This article is a U.S. Government work and is in the public domain in the USA.},
  langid = {english},
  pmid = {24943680},
  keywords = {Humans,matched-pairs design,one-sample t-test,pedagogy,Random Allocation,sign test,Statistics Nonparametric},
  file = {/Users/nathanielyomogida/Zotero/storage/37PML3CQ/Li and Johnson - 2014 - Wilcoxon's signed-rank statistic what null hypoth.pdf}
}

@article{mchughChisquareTestIndependence2013,
  title = {The Chi-Square Test of Independence},
  author = {McHugh, Mary L.},
  year = {2013},
  journal = {Biochemia Medica},
  volume = {23},
  number = {2},
  pages = {143--149},
  issn = {1330-0962},
  doi = {10.11613/bm.2013.018},
  abstract = {The Chi-square statistic is a non-parametric (distribution free) tool designed to analyze group differences when the dependent variable is measured at a nominal level. Like all non-parametric statistics, the Chi-square is robust with respect to the distribution of the data. Specifically, it does not require equality of variances among the study groups or homoscedasticity in the data. It permits evaluation of both dichotomous independent variables, and of multiple group studies. Unlike many other non-parametric and some parametric statistics, the calculations needed to compute the Chi-square provide considerable information about how each of the groups performed in the study. This richness of detail allows the researcher to understand the results and thus to derive more detailed information from this statistic than from many others. The Chi-square is a significance statistic, and should be followed with a strength statistic. The Cramer's V is the most common strength test used to test the data when a significant Chi-square result has been obtained. Advantages of the Chi-square include its robustness with respect to distribution of the data, its ease of computation, the detailed information that can be derived from the test, its use in studies for which parametric assumptions cannot be met, and its flexibility in handling data from both two group and multiple group studies. Limitations include its sample size requirements, difficulty of interpretation when there are large numbers of categories (20 or more) in the independent or dependent variables, and tendency of the Cramer's V to produce relative low correlation measures, even for highly significant results.},
  langid = {english},
  pmcid = {PMC3900058},
  pmid = {23894860},
  keywords = {Chi-Square Distribution,Data Interpretation Statistical,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/4XWHZID4/McHugh (2013) The chi-square test of independence.pdf;/Users/nathanielyomogida/Zotero/storage/RG5M6LH4/mchugh2013.pdf.pdf}
}

@article{mchughInterraterReliabilityKappa2012,
  title = {Interrater Reliability: The Kappa Statistic},
  shorttitle = {Interrater Reliability},
  author = {McHugh, Mary L.},
  year = {2012},
  journal = {Biochemia Medica},
  volume = {22},
  number = {3},
  pages = {276--282},
  issn = {1330-0962},
  abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
  langid = {english},
  pmcid = {PMC3900052},
  pmid = {23092060},
  keywords = {Data Interpretation Statistical,Observer Variation,Reproducibility of Results,To Read}
}

@article{mcmanusMisinterpretationStandardError2012,
  title = {The Misinterpretation of the Standard Error of Measurement in Medical Education: A Primer on the Problems, Pitfalls and Peculiarities of the Three Different Standard Errors of Measurement},
  shorttitle = {The Misinterpretation of the Standard Error of Measurement in Medical Education},
  author = {McManus, I. C.},
  year = {2012},
  journal = {Medical Teacher},
  volume = {34},
  number = {7},
  pages = {569--576},
  issn = {1466-187X},
  doi = {10.3109/0142159X.2012.670318},
  abstract = {BACKGROUND: In high-stakes assessments in medical education, such as final undergraduate examinations and postgraduate assessments, an attempt is frequently made to set confidence limits on the probable true score of a candidate. Typically, this is carried out using what is referred to as the standard error of measurement (SEM). However, it is often the case that the wrong formula is applied, there actually being three different formulae for use in different situations. AIMS: To explain and clarify the calculation of the SEM, and differentiate three separate standard errors, which here are called the standard error of measurement (SEmeas), the standard error of estimation (SEest) and the standard error of prediction (SEpred). RESULTS: Most accounts describe the calculation of SEmeas. For most purposes, though, what is required is the standard error of estimation (SEest), which has to be applied not to a candidate's actual score but to their estimated true score after taking into account the regression to the mean that occurs due to the unreliability of an assessment. A third formula, the standard error of prediction (SEpred) is less commonly used in medical education, but is useful in situations such as counselling, where one needs to predict a future actual score on an examination from a previous actual score on the same examination. CONCLUSIONS: The various formulae can produce predictions that differ quite substantially, particularly when reliability is not particularly high, and the mark in question is far removed from the average performance of candidates. That can have important, unintended consequences, particularly in a medico-legal context.},
  langid = {english},
  pmid = {22509894},
  keywords = {Education Medical,Educational Measurement,Humans,Models Statistical,Reproducibility of Results,To Read,United Kingdom},
  file = {/Users/nathanielyomogida/Zotero/storage/AQDRHGUT/McManus (2012) The misinterpretation of the standard error of measurement in medical education.pdf}
}

@article{monaghanFoundationalStatisticalPrinciples2021,
  title = {Foundational {{Statistical Principles}} in {{Medical Research}}: {{Sensitivity}}, {{Specificity}}, {{Positive Predictive Value}}, and {{Negative Predictive Value}}},
  shorttitle = {Foundational {{Statistical Principles}} in {{Medical Research}}},
  author = {Monaghan, Thomas F. and Rahman, Syed N. and Agudelo, Christina W. and Wein, Alan J. and Lazar, Jason M. and Everaert, Karel and Dmochowski, Roger R.},
  year = {2021},
  month = may,
  journal = {Medicina (Kaunas, Lithuania)},
  volume = {57},
  number = {5},
  pages = {503},
  issn = {1648-9144},
  doi = {10.3390/medicina57050503},
  abstract = {Sensitivity, which denotes the proportion of subjects correctly given a positive assignment out of all subjects who are actually positive for the outcome, indicates how well a test can classify subjects who truly have the outcome of interest. Specificity, which denotes the proportion of subjects correctly given a negative assignment out of all subjects who are actually negative for the outcome, indicates how well a test can classify subjects who truly do not have the outcome of interest. Positive predictive value reflects the proportion of subjects with a positive test result who truly have the outcome of interest. Negative predictive value reflects the proportion of subjects with a negative test result who truly do not have the outcome of interest. Sensitivity and specificity are inversely related, wherein one increases as the other decreases, but are generally considered stable for a given test, whereas positive and negative predictive values do inherently vary with pre-test probability (e.g., changes in population disease prevalence). This article will further detail the concepts of sensitivity, specificity, and predictive values using a recent real-world example from the medical literature.},
  langid = {english},
  pmcid = {PMC8156826},
  pmid = {34065637},
  keywords = {basics,Biomedical Research,biostatistics,diagnosis,Finish reading,fundamentals,Humans,introduction,methodology,overview,Predictive Value of Tests,Prevalence,screening,Sensitivity and Specificity,statistics,tutorial},
  file = {/Users/nathanielyomogida/Zotero/storage/9VXPL2UH/Monaghan et al (2021) Foundational Statistical Principles in Medical Research.pdf;/Users/nathanielyomogida/Zotero/storage/YV92UZP5/monaghan2021.pdf.pdf}
}

@article{monaghanFoundationalStatisticalPrinciples2021a,
  title = {Foundational {{Statistical Principles}} in {{Medical Research}}: {{Sensitivity}}, {{Specificity}}, {{Positive Predictive Value}}, and {{Negative Predictive Value}}},
  shorttitle = {Foundational {{Statistical Principles}} in {{Medical Research}}},
  author = {Monaghan, Thomas F. and Rahman, Syed N. and Agudelo, Christina W. and Wein, Alan J. and Lazar, Jason M. and Everaert, Karel and Dmochowski, Roger R.},
  year = {2021},
  month = may,
  journal = {Medicina (Kaunas, Lithuania)},
  volume = {57},
  number = {5},
  pages = {503},
  issn = {1648-9144},
  doi = {10.3390/medicina57050503},
  abstract = {Sensitivity, which denotes the proportion of subjects correctly given a positive assignment out of all subjects who are actually positive for the outcome, indicates how well a test can classify subjects who truly have the outcome of interest. Specificity, which denotes the proportion of subjects correctly given a negative assignment out of all subjects who are actually negative for the outcome, indicates how well a test can classify subjects who truly do not have the outcome of interest. Positive predictive value reflects the proportion of subjects with a positive test result who truly have the outcome of interest. Negative predictive value reflects the proportion of subjects with a negative test result who truly do not have the outcome of interest. Sensitivity and specificity are inversely related, wherein one increases as the other decreases, but are generally considered stable for a given test, whereas positive and negative predictive values do inherently vary with pre-test probability (e.g., changes in population disease prevalence). This article will further detail the concepts of sensitivity, specificity, and predictive values using a recent real-world example from the medical literature.},
  langid = {english},
  pmcid = {PMC8156826},
  pmid = {34065637},
  keywords = {basics,Biomedical Research,biostatistics,Completed,diagnosis,fundamentals,Humans,introduction,methodology,overview,Predictive Value of Tests,Prevalence,screening,Sensitivity and Specificity,statistics,tutorial},
  file = {/Users/nathanielyomogida/Zotero/storage/VCNXTX4D/Monaghan et al (2021) Foundational Statistical Principles in Medical Research.pdf}
}

@article{muradHowReadSystematic2014,
  title = {How to Read a Systematic Review and Meta-Analysis and Apply the Results to Patient Care: Users' Guides to the Medical Literature},
  shorttitle = {How to Read a Systematic Review and Meta-Analysis and Apply the Results to Patient Care},
  author = {Murad, Mohammad Hassan and Montori, Victor M. and Ioannidis, John P. A. and Jaeschke, Roman and Devereaux, P. J. and Prasad, Kameshwar and Neumann, Ignacio and {Carrasco-Labra}, Alonso and Agoritsas, Thomas and Hatala, Rose and Meade, Maureen O. and Wyer, Peter and Cook, Deborah J. and Guyatt, Gordon},
  year = {2014},
  month = jul,
  journal = {JAMA},
  volume = {312},
  number = {2},
  pages = {171--179},
  issn = {1538-3598},
  doi = {10.1001/jama.2014.5559},
  abstract = {Clinical decisions should be based on the totality of the best evidence and not the results of individual studies. When clinicians apply the results of a systematic review or meta-analysis to patient care, they should start by evaluating the credibility of the methods of the systematic review, ie, the extent to which these methods have likely protected against misleading results. Credibility depends on whether the review addressed a sensible clinical question; included an exhaustive literature search; demonstrated reproducibility of the selection and assessment of studies; and presented results in a useful manner. For reviews that are sufficiently credible, clinicians must decide on the degree of confidence in the estimates that the evidence warrants (quality of evidence). Confidence depends on the risk of bias in the body of evidence; the precision and consistency of the results; whether the results directly apply to the patient of interest; and the likelihood of reporting bias. Shared decision making requires understanding of the estimates of magnitude of beneficial and harmful effects, and confidence in those estimates.},
  langid = {english},
  pmid = {25005654},
  keywords = {Bias,Decision Making,Evidence-Based Practice,Humans,Meta-Analysis as Topic,Patient Care,Reproducibility of Results,Systematic Reviews as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/WDX2AD68/Murad et al (2014) How to read a systematic review and meta-analysis and apply the results to.pdf}
}

@article{nahmReceiverOperatingCharacteristic2022,
  title = {Receiver Operating Characteristic Curve: Overview and Practical Use for Clinicians},
  shorttitle = {Receiver Operating Characteristic Curve},
  author = {Nahm, Francis Sahngun},
  year = {2022},
  month = feb,
  journal = {Korean Journal of Anesthesiology},
  volume = {75},
  number = {1},
  pages = {25--36},
  issn = {2005-7563},
  doi = {10.4097/kja.21209},
  abstract = {Using diagnostic testing to determine the presence or absence of a disease is essential in clinical practice. In many cases, test results are obtained as continuous values and require a process of conversion and interpretation and into a dichotomous form to determine the presence of a disease. The primary method used for this process is the receiver operating characteristic (ROC) curve. The ROC curve is used to assess the overall diagnostic performance of a test and to compare the performance of two or more diagnostic tests. It is also used to select an optimal cut-off value for determining the presence or absence of a disease. Although clinicians who do not have expertise in statistics do not need to understand both the complex mathematical equation and the analytic process of ROC curves, understanding the core concepts of the ROC curve analysis is a prerequisite for the proper use and interpretation of the ROC curve. This review describes the basic concepts for the correct use and interpretation of the ROC curve, including parametric/nonparametric ROC curves, the meaning of the area under the ROC curve (AUC), the partial AUC, methods for selecting the best cut-off value, and the statistical software to use for ROC curve analyses.},
  langid = {english},
  pmcid = {PMC8831439},
  pmid = {35124947},
  keywords = {Area under curve,Humans,Mathematics,Reference values,Research design,Research Design,ROC curve,ROC Curve,Routine diagnostic tests,Statistics},
  file = {/Users/nathanielyomogida/Zotero/storage/47M6AC4C/Nahm (2022) Receiver operating characteristic curve.pdf}
}

@article{nicholsControllingFamilywiseError2003,
  title = {Controlling the Familywise Error Rate in Functional Neuroimaging: A Comparative Review},
  shorttitle = {Controlling the Familywise Error Rate in Functional Neuroimaging},
  author = {Nichols, Thomas and Hayasaka, Satoru},
  year = {2003},
  month = oct,
  journal = {Statistical Methods in Medical Research},
  volume = {12},
  number = {5},
  pages = {419--446},
  issn = {0962-2802},
  doi = {10.1191/0962280203sm341ra},
  abstract = {Functional neuroimaging data embodies a massive multiple testing problem, where 100,000 correlated test statistics must be assessed. The familywise error rate, the chance of any false positives is the standard measure of Type I errors in multiple testing. In this paper we review and evaluate three approaches to thresholding images of test statistics: Bonferroni, random field and the permutation test. Owing to recent developments, improved Bonferroni procedures, such as Hochberg's methods, are now applicable to dependent data. Continuous random field methods use the smoothness of the image to adapt to the severity of the multiple testing problem. Also, increased computing power has made both permutation and bootstrap methods applicable to functional neuroimaging. We evaluate these approaches on t images using simulations and a collection of real datasets. We find that Bonferroni-related tests offer little improvement over Bonferroni, while the permutation method offers substantial improvement over the random field method for low smoothness and low degrees of freedom. We also show the limitations of trying to find an equivalent number of independent tests for an image of correlated test statistics.},
  langid = {english},
  pmid = {14599004},
  keywords = {Brain,Data Interpretation Statistical,Humans,Magnetic Resonance Imaging,Models Statistical,Sensitivity and Specificity,Stochastic Processes,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/HIX28XLX/Nichols_Hayasaka (2003) Controlling the familywise error rate in functional neuroimaging.pdf}
}

@article{nicholsonRoleFamilywiseError2022,
  title = {The {{Role}} of {{Family-wise Error Rate}} in {{Determining Statistical Significance}}},
  author = {Nicholson, Kristen J. and Sherman, Matthew and Divi, Srikanth N. and Bowles, Daniel R. and Vaccaro, Alex R.},
  year = {2022},
  month = jun,
  journal = {Clinical Spine Surgery},
  volume = {35},
  number = {5},
  pages = {222--223},
  issn = {2380-0194},
  doi = {10.1097/BSD.0000000000001287},
  abstract = {The threshold for statistical significance is determined by the maximum allowable probability of Type I error ({$\alpha$}). For studies that test multiple hypotheses or make multiple comparisons, the probability of at least 1 Type I error (family-wise error rate; FWER) increases as the number of hypotheses/comparisons increase. It is generally best practice to set the acceptable threshold for FWER to be less than or equal to {$\alpha$}. Bonferroni correction and Tukey honestly significant difference test are 2 of the more common methods to control for FWER. When doing exploratory analysis or evaluating secondary outcomes of a study, it may not be necessary or desirable to control for FWER, which reduces the power of the study. However, deciding to control for FWER should be decided during the design of the study.},
  langid = {english},
  pmid = {34907926},
  keywords = {Finish reading,Humans,Probability}
}

@article{nunanCatalogueBiasAttrition2018,
  title = {Catalogue of Bias: Attrition Bias},
  shorttitle = {Catalogue of Bias},
  author = {Nunan, David and Aronson, Jeffrey and Bankhead, Clare},
  year = {2018},
  month = feb,
  journal = {BMJ evidence-based medicine},
  volume = {23},
  number = {1},
  pages = {21--22},
  issn = {2515-4478},
  doi = {10.1136/ebmed-2017-110883},
  abstract = {This article is part of a series of articles featuring the Catalogue of Bias introduced in this volume of BMJ Evidence-Based Medicine that describes attrition bias and outlines its potential impact on research studies and the preventive steps to minimise its risk. Attrition bias is a type of selection bias due to systematic differences between study groups in the number and the way participants are lost from a study. Differences between people who leave a study and those who continue, particularly between study groups, can be the reason for any observed effect and not the intervention itself. Associations for mortality in trials of tranexamic acid and upper gastrointestinal bleeding were no longer apparent after studies with high or unclear risk of attrition bias were removed. Over-recruitment can help prevent important attrition bias. Sampling weights and tailored replenishment samples can help to compensate for the effects of attrition bias when present.},
  langid = {english},
  pmid = {29367321},
  keywords = {Bias,clinical trials,Completed,epidemiology,Humans,Patient Dropouts,Selection Bias},
  file = {/Users/nathanielyomogida/Zotero/storage/Y7NITXLK/Nunan et al (2018) Catalogue of bias.pdf}
}

@article{politAssessingMeasurementHealth2015,
  title = {Assessing Measurement in Health: {{Beyond}} Reliability and Validity},
  shorttitle = {Assessing Measurement in Health},
  author = {Polit, Denise F.},
  year = {2015},
  month = nov,
  journal = {International Journal of Nursing Studies},
  volume = {52},
  number = {11},
  pages = {1746--1753},
  issn = {1873-491X},
  doi = {10.1016/j.ijnurstu.2015.07.002},
  abstract = {BACKGROUND: Psychometric concepts have undergone a transformation in health fields, as articulated in a consensus report by an international panel of health measurement experts: COSMIN, the COnsensus-based Standards for the selection of health Measurement INstruments. OBJECTIVES: The aims of this paper are to describe emerging ideas relating to the development and testing of new measures in health fields, to present a revised measurement taxonomy that builds upon COSMIN, and to explore the extent to which the new measurement concepts have played a role in psychometric assessments in nursing. DESIGN: A descriptive analysis of a sample of psychometric papers published in three major nursing journals was undertaken. METHODS: A new measurement taxonomy is presented and explained. A sample of 105 studies, representing a consecutive sample of psychometric studies published in the International Journal of Nursing Studies, Nursing Research, and Research in Nursing \& Health between 2010 and 2014 was reviewed to ascertain the extent to which psychometric assessments in nursing map onto the new taxonomy. RESULTS: Most nursing studies reviewed adhered to traditional concepts of psychometric assessment, which focus on reliability and validity. The studies in the sample rarely involved assessments of longitudinal measurement aspects, namely the reliability and validity of change scores (responsiveness). CONCLUSIONS: Many constructs of interest to nurse researchers are amenable to change-and these constructs are frequently the target of nursing interventions designed to foster change. Future psychometric work by nurse researchers would benefit from assessments of the psychometric adequacy of change scores.},
  langid = {english},
  pmid = {26234936},
  keywords = {Change scores,Classification,COSMIN,Health,Humans,Instrument development,Measurement,Measurement error,Psychometrics,Reliability,Reproducibility of Results,Research Design,Responsiveness,Scale development,Validity},
  file = {/Users/nathanielyomogida/Zotero/storage/4TMT2W2A/Polit - 2015 - Assessing measurement in health Beyond reliabilit.pdf}
}

@article{rejasStandardErrorMeasurement2008,
  title = {Standard Error of Measurement as a Valid Alternative to Minimally Important Difference for Evaluating the Magnitude of Changes in Patient-Reported Outcomes Measures},
  author = {Rejas, Javier and Pardo, Antonio and Ruiz, Miguel Angel},
  year = {2008},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {61},
  number = {4},
  pages = {350--356},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2007.05.011},
  abstract = {OBJECTIVE: To assess the degree of agreement between standard error of measurement (SEM) and minimally important difference (MID) criteria to evaluate the magnitude of the change caused by a medical intervention. STUDY DESIGN AND SETTING: Data were obtained from a cohort of 603 patients with neuropathic pain undergoing analgesic treatment with gabapentin who completed four health scales: Medical Outcomes Study Sleep Scale, Sheehan Disability Scale, Covi Anxiety Scale, and Raskin Depression scale. After calculating MID and SEM values for all scales, patients were classified into three categories: improvement, no change, and worsening. Agreement between the two criteria was assessed using Cohen's kappa index of agreement and Kendall's tau-b linear correlation coefficient. RESULTS: The 1 SEM criterion showed the highest agreement (kappa=0.68-1.00) and correlation (tau-b=0.75-1.00) with the MID criterion. Sensitivity analysis performed in gabapentin responders and nonresponders confirmed the results of the main analysis. CONCLUSION: The 1 SEM criterion is a valid alternative to the MID criterion to evaluate the magnitude of the change produced in patient-reported health outcomes measures.},
  langid = {english},
  pmid = {18313559},
  keywords = {Amines,Analgesics,Cohort Studies,Cyclohexanecarboxylic Acids,Data Interpretation Statistical,Gabapentin,gamma-Aminobutyric Acid,Humans,Pain,Patient Satisfaction,To Read,Treatment Outcome},
  file = {/Users/nathanielyomogida/Zotero/storage/KNLJNRHK/Rejas et al (2008) Standard error of measurement as a valid alternative to minimally important.pdf}
}

@article{rigattiRandomForest2017,
  title = {Random {{Forest}}},
  author = {Rigatti, Steven J.},
  year = {2017},
  journal = {Journal of Insurance Medicine (New York, N.Y.)},
  volume = {47},
  number = {1},
  pages = {31--39},
  issn = {0743-6661},
  doi = {10.17849/insm-47-01-31-39.1},
  abstract = {For the task of analyzing survival data to derive risk factors associated with mortality, physicians, researchers, and biostatisticians have typically relied on certain types of regression techniques, most notably the Cox model. With the advent of more widely distributed computing power, methods which require more complex mathematics have become increasingly common. Particularly in this era of "big data" and machine learning, survival analysis has become methodologically broader. This paper aims to explore one technique known as Random Forest. The Random Forest technique is a regression tree technique which uses bootstrap aggregation and randomization of predictors to achieve a high degree of predictive accuracy. The various input parameters of the random forest are explored. Colon cancer data (n = 66,807) from the SEER database is then used to construct both a Cox model and a random forest model to determine how well the models perform on the same data. Both models perform well, achieving a concordance error rate of approximately 18\%.},
  langid = {english},
  pmid = {28836909},
  keywords = {Completed,Machine Learning,Random forest,Regression Analysis,Risk Factors,Survival Analysis},
  file = {/Users/nathanielyomogida/Zotero/storage/FW398KL7/Rigatti (2017) Random Forest.pdf}
}

@article{robertsonGraphicalApproachesControl2020,
  title = {Graphical Approaches for the Control of Generalized Error Rates},
  author = {Robertson, David S. and Wason, James M. S. and Bretz, Frank},
  year = {2020},
  month = oct,
  journal = {Statistics in Medicine},
  volume = {39},
  number = {23},
  pages = {3135--3155},
  issn = {1097-0258},
  doi = {10.1002/sim.8595},
  abstract = {When simultaneously testing multiple hypotheses, the usual approach in the context of confirmatory clinical trials is to control the familywise error rate (FWER), which bounds the probability of making at least one false rejection. In many trial settings, these hypotheses will additionally have a hierarchical structure that reflects the relative importance and links between different clinical objectives. The graphical approach of Bretz et al~(2009) is a flexible and easily communicable way of controlling the FWER while respecting complex trial objectives and multiple structured hypotheses. However, the FWER can be a very stringent criterion that leads to procedures with low power, and may not be appropriate in exploratory trial settings. This motivates controlling generalized error rates, particularly when the number of hypotheses tested is no longer small. We consider the generalized familywise error rate (k-FWER), which is the probability of making~k or more false rejections, as well as the tail probability of the false discovery proportion (FDP), which is the probability that the proportion of false rejections is greater than some threshold. We also consider asymptotic control of the false discovery rate, which is the expectation of the FDP. In this article, we show how to control these generalized error rates when using the graphical approach and its extensions. We demonstrate the utility of the resulting graphical procedures on three clinical trial case studies.},
  langid = {english},
  pmcid = {PMC7612110},
  pmid = {32557848},
  keywords = {false discovery proportion,generalized familywise error rate,Humans,hypothesis testing,multiple comparison procedures,multiple endpoints,Probability,Research Design,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/6BCGMFYR/Robertson et al (2020) Graphical approaches for the control of generalized error rates.pdf}
}

@article{robertsResearchTechniquesMade2019,
  title = {Research {{Techniques Made Simple}}: {{Interpreting Measures}}~of {{Association}} in {{Clinical Research}}},
  shorttitle = {Research {{Techniques Made Simple}}},
  author = {Roberts, Michelle R. and Ashrafzadeh, Sepideh and Asgari, Maryam M.},
  year = {2019},
  month = mar,
  journal = {The Journal of Investigative Dermatology},
  volume = {139},
  number = {3},
  pages = {502-511.e1},
  issn = {1523-1747},
  doi = {10.1016/j.jid.2018.12.023},
  abstract = {To bring evidence-based improvements in medicine and health care delivery to clinical practice, health care providers must know how to interpret clinical research findings and critically evaluate the strength of evidence. This requires an understanding of differences in clinical study designs and the various statistical methods used to identify associations. We aim to provide a foundation for understanding the common measures of association used in epidemiologic studies to quantify relationships between exposures and outcomes, including relative risks, odds ratios, and hazard ratios. We also provide a framework for critically assessing clinical research findings and highlight specific methodologic concerns.},
  langid = {english},
  pmcid = {PMC7737849},
  pmid = {30797315},
  keywords = {Biomedical Research,Delivery of Health Care,Education Medical Continuing,Epidemiologic Methods,Evidence-Based Medicine,Finish reading,Humans,Models Statistical,Odds Ratio,Quality Improvement,Research Design},
  file = {/Users/nathanielyomogida/Zotero/storage/N6RNRH9I/Roberts et al (2019) Research Techniques Made Simple.pdf}
}

@article{rosnowDefinitionInterpretationInteraction1989,
  title = {Definition and Interpretation of Interaction Effects.},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {1},
  pages = {143--146},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.105.1.143},
  url = {https://doi.apa.org/doi/10.1037/0033-2909.105.1.143},
  urldate = {2024-02-27},
  langid = {english},
  keywords = {To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/5VKJZW9A/Rosnow_Rosenthal (1989) Definition and interpretation of interaction effects.pdf}
}

@article{saaiqModifyingPicoQuestion2017,
  title = {Modifying "{{Pico}}" {{Question}} into "{{Picos}}" {{Model}} for {{More Robust}} and {{Reproducible Presentation}} of the {{Methodology Employed}} in {{A Scientific Study}}},
  author = {Saaiq, Muhammad and Ashraf, Bushra},
  year = {2017},
  month = sep,
  journal = {World Journal of Plastic Surgery},
  volume = {6},
  number = {3},
  pages = {390--392},
  issn = {2228-7914},
  langid = {english},
  pmcid = {PMC5714990},
  pmid = {29218294},
  keywords = {Methodology,Pico,Picos,Presentation,Study},
  file = {/Users/nathanielyomogida/Zotero/storage/4DBB7VC8/Saaiq_Ashraf (2017) Modifying Pico Question into Picos Model for More Robust and Reproducible.pdf}
}

@article{schoberCorrelationCoefficientsAppropriate2018,
  title = {Correlation {{Coefficients}}: {{Appropriate Use}} and {{Interpretation}}},
  shorttitle = {Correlation {{Coefficients}}},
  author = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
  year = {2018},
  month = may,
  journal = {Anesthesia and Analgesia},
  volume = {126},
  number = {5},
  pages = {1763--1768},
  issn = {1526-7598},
  doi = {10.1213/ANE.0000000000002864},
  abstract = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from -1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
  langid = {english},
  pmid = {29481436},
  keywords = {Completed,Correlation of Data,Data Collection,Data Interpretation Statistical,Humans,Statistics Nonparametric},
  file = {/Users/nathanielyomogida/Zotero/storage/DIRTFDG8/Schober et al. - 2018 - Correlation Coefficients Appropriate Use and Inte.pdf}
}

@article{scurlock-evansEvidencebasedPracticePhysiotherapy2014,
  title = {Evidence-Based Practice in Physiotherapy: A Systematic Review of Barriers, Enablers and Interventions},
  shorttitle = {Evidence-Based Practice in Physiotherapy},
  author = {{Scurlock-Evans}, Laura and Upton, Penney and Upton, Dominic},
  year = {2014},
  month = sep,
  journal = {Physiotherapy},
  volume = {100},
  number = {3},
  pages = {208--219},
  issn = {1873-1465},
  doi = {10.1016/j.physio.2014.03.001},
  abstract = {BACKGROUND: Despite clear benefits of the Evidence-Based Practice (EBP) approach to ensuring quality and consistency of care, its uptake within physiotherapy has been inconsistent. OBJECTIVES: Synthesise the findings of research into EBP barriers, facilitators and interventions in physiotherapy and identify methods of enhancing adoption and implementation. DATA SOURCES: Literature concerning physiotherapists' practice between 2000 and 2012 was systematically searched using: Academic Search Complete, Cumulative Index of Nursing and Allied Health Literature Plus, American Psychological Association databases, Medline, Journal Storage, and Science Direct. Reference lists were searched to identify additional studies. STUDY SELECTION: Thirty-two studies, focusing either on physiotherapists' EBP knowledge, attitudes or implementation, or EBP interventions in physiotherapy were included. DATA EXTRACTION AND SYNTHESIS: One author undertook all data extraction and a second author reviewed to ensure consistency and rigour. Synthesis was organised around the themes of EBP barriers/enablers, attitudes, knowledge/skills, use and interventions. RESULTS: Many physiotherapists hold positive attitudes towards EBP. However, this does not necessarily translate into consistent, high-quality EBP. Many barriers to EBP implementation are apparent, including: lack of time and skills, and misperceptions of EBP. LIMITATIONS: Only studies published in the English language, in peer-reviewed journals were included, thereby introducing possible publication bias. Furthermore, narrative synthesis may be subject to greater confirmation bias. CONCLUSION AND IMPLICATIONS: There is no "one-size fits all" approach to enhancing EBP implementation; assessing organisational culture prior to designing interventions is crucial. Although some interventions appear promising, further research is required to explore the most effective methods of supporting physiotherapists' adoption of EBP.},
  langid = {english},
  pmid = {24780633},
  keywords = {Best Practices,Decision Making,Evidence-Based Practice,Health Knowledge Attitudes Practice,Humans,Physical Therapy Modalities,Physiotherapists,Practice-Research Gap,Quality Assurance Health Care,Review},
  file = {/Users/nathanielyomogida/Zotero/storage/6LSQRZGB/Scurlock-Evans et al (2014) Evidence-based practice in physiotherapy.pdf}
}

@article{sedighiInterpretationDiagnosticTests2013,
  title = {Interpretation of {{Diagnostic Tests}}: {{Likelihood Ratio}} vs. {{Predictive Value}}},
  shorttitle = {Interpretation of {{Diagnostic Tests}}},
  author = {Sedighi, Iraj},
  year = {2013},
  month = dec,
  journal = {Iranian Journal of Pediatrics},
  volume = {23},
  number = {6},
  pages = {717},
  issn = {2008-2142},
  langid = {english},
  pmcid = {PMC4025141},
  pmid = {24910762},
  keywords = {Finish reading}
}

@article{shafferControllingFalseDiscovery2007,
  title = {Controlling the False Discovery Rate with Constraints: The {{Newman-Keuls}} Test Revisited},
  shorttitle = {Controlling the False Discovery Rate with Constraints},
  author = {Shaffer, Juliet Popper},
  year = {2007},
  month = feb,
  journal = {Biometrical Journal. Biometrische Zeitschrift},
  volume = {49},
  number = {1},
  pages = {136--143},
  issn = {0323-3847},
  doi = {10.1002/bimj.200610297},
  abstract = {The Newman-Keuls (NK) procedure for testing all pairwise comparisons among a set of treatment means, introduced by Newman (1939) and in a slightly different form by Keuls (1952) was proposed as a reasonable way to alleviate the inflation of error rates when a large number of means are compared. It was proposed before the concepts of different types of multiple error rates were introduced by Tukey (1952a, b; 1953). Although it was popular in the 1950s and 1960s, once control of the familywise error rate (FWER) was accepted generally as an appropriate criterion in multiple testing, and it was realized that the NK procedure does not control the FWER at the nominal level at which it is performed, the procedure gradually fell out of favor. Recently, a more liberal criterion, control of the false discovery rate (FDR), has been proposed as more appropriate in some situations than FWER control. This paper notes that the NK procedure and a nonparametric extension controls the FWER within any set of homogeneous treatments. It proves that the extended procedure controls the FDR when there are well-separated clusters of homogeneous means and between-cluster test statistics are independent, and extensive simulation provides strong evidence that the original procedure controls the FDR under the same conditions and some dependent conditions when the clusters are not well-separated. Thus, the test has two desirable error-controlling properties, providing a compromise between FDR control with no subgroup FWER control and global FWER control. Yekutieli (2002) developed an FDR-controlling procedure for testing all pairwise differences among means, without any FWER-controlling criteria when there is more than one cluster. The empirica example in Yekutieli's paper was used to compare the Benjamini-Hochberg (1995) method with apparent FDR control in this context, Yekutieli's proposed method with proven FDR control, the Newman-Keuls method that controls FWER within equal clusters with apparent FDR control, and several methods that control FWER globally. The Newman-Keuls is shown to be intermediate in number of rejections to the FWER-controlling methods and the FDR-controlling methods in this example, although it is not always more conservative than the other FDR-controlling methods.},
  langid = {english},
  pmid = {17342955},
  keywords = {Data Interpretation Statistical,Matched-Pair Analysis,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/9WV2HZTT/Shaffer (2007) Controlling the false discovery rate with constraints.pdf}
}

@incollection{shrefflerDiagnosticTestingAccuracy2024,
  title = {Diagnostic {{Testing Accuracy}}: {{Sensitivity}}, {{Specificity}}, {{Predictive Values}} and {{Likelihood Ratios}}},
  shorttitle = {Diagnostic {{Testing Accuracy}}},
  booktitle = {{{StatPearls}}},
  author = {Shreffler, Jacob and Huecker, Martin R.},
  year = {2024},
  publisher = {StatPearls Publishing},
  address = {Treasure Island (FL)},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK557491/},
  urldate = {2024-01-27},
  abstract = {To make clinical decisions and guide patient care, providers must comprehend the likelihood of a patient having a disease, combining an understanding of pretest probability and diagnostic assessments.~Diagnostic tools are routinely utilized in healthcare settings to determine treatment methods; however, many of these tools are subject to error.},
  copyright = {Copyright {\copyright} 2024, StatPearls Publishing LLC.},
  langid = {english},
  lccn = {NBK557491},
  pmid = {32491423},
  keywords = {To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/S5E3VLF3/NBK557491.html}
}

@article{simMeasurementValidityPhysical1993,
  title = {Measurement Validity in Physical Therapy Research},
  author = {Sim, J. and Arnell, P.},
  year = {1993},
  month = feb,
  journal = {Physical Therapy},
  volume = {73},
  number = {2},
  pages = {102-110; discussion 110-115},
  issn = {0031-9023},
  doi = {10.1093/ptj/73.2.102},
  abstract = {This article considers the role of measurement validity within physical therapy research. The concept of measurement validity is identified as a component of internal validity, and it is differentiated from the notion of reliability; these concepts are related to systematic and random sources of error, respectively. Using examples from physical therapy and rehabilitation, four main types of validity are reviewed: face validity, criterion-related validity, content validity, and construct validity. The differing implications of these types of validity for quantitative and qualitative research are discussed. Three principal areas of concern are then addressed, based on a critical discussion of selected examples from the literature. First, it is argued that validity is often poorly distinguished from the allied concept of reliability and that purported claims for validity often only demonstrate reliability. Second, it is claimed that validity is too often neglected in favor of reliability, and specific examples relating to gait analysis are put forward to support this argument. Third, some of the methodological difficulties that may occur when attempts are made to demonstrate validity are considered. The article concludes with a plea for a closer focus on the issue of measurement validity within physical therapy research.},
  langid = {english},
  pmid = {8421716},
  keywords = {Bias,Finish reading,Humans,Models Organizational,Physical Therapy Modalities,Reproducibility of Results,Research,Research Design},
  file = {/Users/nathanielyomogida/Zotero/storage/KGZMHCPG/Sim and Arnell - 1993 - Measurement validity in physical therapy research.pdf}
}

@article{souzaPropriedadesPsicometricasNa2017,
  title = {{Propriedades psicom{\'e}tricas na avalia{\c c}{\~a}o de instrumentos: avalia{\c c}{\~a}o da confiabilidade e da validade}},
  shorttitle = {{Propriedades psicom{\'e}tricas na avalia{\c c}{\~a}o de instrumentos}},
  author = {Souza, Ana Cl{\'a}udia De and Alexandre, Neusa Maria Costa and Guirardello, Edin{\^e}is De Brito and Souza, Ana Cl{\'a}udia De and Alexandre, Neusa Maria Costa and Guirardello, Edin{\^e}is De Brito},
  year = {2017},
  month = jul,
  journal = {Epidemiologia e Servi{\c c}os de Sa{\'u}de},
  volume = {26},
  number = {3},
  pages = {649--659},
  issn = {1679-4974},
  doi = {10.5123/S1679-49742017000300022},
  url = {http://revista.iec.gov.br/template_doi_ess.php?doi=10.5123/S1679-49742017000300649&scielo=S2237-96222017000300649},
  urldate = {2024-04-12},
  langid = {portuguese}
}

@article{streinerInternalExternalValidityStatisticsCommentarySeries2020,
  title = {Statistics {{Commentary Series}}. {{Commentary No}}. 44: {{Internal}} and {{External Validity}}},
  shorttitle = {Statistics {{Commentary Series}}. {{Commentary No}}. 44},
  author = {Streiner, David L.},
  year = {2020},
  journal = {Journal of Clinical Psychopharmacology},
  volume = {40},
  number = {6},
  pages = {531--533},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000001304},
  langid = {english},
  pmid = {33044353},
  keywords = {Data Interpretation Statistical,Humans,Models Statistical,Randomized Controlled Trials as Topic,Reproducibility of Results,Research Design}
}

@article{streinerMeasuringChangeStatisticsCommentarySeries2018,
  title = {Statistics {{Commentary Series}}: {{Commentary No}}. 25: {{Measuring Change}}},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2018},
  month = apr,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {38},
  number = {2},
  pages = {111--112},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000849},
  langid = {english},
  pmid = {29389778},
  keywords = {Humans,Mental Disorders,Outcome Assessment Health Care,Statistics as Topic,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/ACE8BGWE/Streiner (2018) Statistics Commentary Series.pdf;/Users/nathanielyomogida/Zotero/storage/ZITWMZJA/streiner2018.pdf.pdf}
}

@article{streinerRegressionMeanIts2001,
  title = {Regression toward the Mean: Its Etiology, Diagnosis, and Treatment},
  shorttitle = {Regression toward the Mean},
  author = {Streiner, D. L.},
  year = {2001},
  month = feb,
  journal = {Canadian Journal of Psychiatry. Revue Canadienne De Psychiatrie},
  volume = {46},
  number = {1},
  pages = {72--76},
  issn = {0706-7437},
  doi = {10.1177/070674370104600111},
  abstract = {This paper explores the phenomenon of "regression toward the mean." The primary effect of this is to affect scores on retesting so that they are closer to the population mean. Thus, people who are selected for inclusion in a study because their scores on some measure are above (or below) some criterion have values on retesting that are less extreme. This may make it appear that the study participants have improved; this will occur even in the absence of an effective intervention. We explore the reasons for regression toward the mean and how it can be detected and discuss some methods that may minimize its effects.},
  langid = {english},
  pmid = {11221493},
  keywords = {Bias,Depressive Disorder,Humans,Personality Inventory,Psychometrics,Reference Values,Regression Analysis,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/5PCC95Z9/Streiner (2001) Regression toward the mean.pdf}
}

@article{streinerRegressionOfTheMean2016,
  title = {Statistics {{Commentary Series}}: {{Commentary}} \#16-{{Regression Toward}} the {{Mean}}},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2016},
  month = oct,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {36},
  number = {5},
  pages = {416--418},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000551},
  langid = {english},
  pmid = {27496345},
  keywords = {Completed,Humans,Psychometrics,Statistical Distributions,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/ZK58FBPF/Streiner (2016) Statistics Commentary Series.pdf}
}

@article{streinerReliabilityStatisticsCommentarySeries2016a,
  title = {Statistics {{Commentary Series}}: {{Commentary}} \#15-{{Reliability}}},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2016},
  month = aug,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {36},
  number = {4},
  pages = {305--307},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000517},
  langid = {english},
  pmid = {27219092},
  keywords = {Completed,Humans,Psychometrics,Reproducibility of Results,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/9JX4IH73/Streiner (2016) Statistics Commentary Series.pdf}
}

@article{streinerStatisticsCommentarySeries2015,
  title = {Statistics {{Commentary Series}}: {{Commentary}} \#9-{{Sample Size Made Easy}} ({{Power}} a {{Bit Less So}})},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2015},
  month = aug,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {35},
  number = {4},
  pages = {364--366},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000297},
  langid = {english},
  pmid = {25769094},
  keywords = {Humans,Randomized Controlled Trials as Topic,Sample Size,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/9N4CDWJ3/Streiner (2015) Statistics Commentary Series.pdf}
}

@article{streinerValidityStatisticsCommentarySeries2016,
  title = {Statistics {{Commentary Series}}: {{Commentary No}}. 17-{{Validity}}},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2016},
  month = dec,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {36},
  number = {6},
  pages = {542--544},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000589},
  langid = {english},
  pmid = {27684292},
  keywords = {Humans,Psychometrics,Reproducibility of Results,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/VD8MRM4F/Streiner (2016) Statistics Commentary Series.pdf}
}

@incollection{sundjajaMcNemarMannWhitneyTests2023,
  title = {{{McNemar And Mann-Whitney U Tests}}},
  booktitle = {{{StatPearls}}},
  author = {Sundjaja, Joshua Henrina and Shrestha, Rijen and Krishan, Kewal},
  year = {2023},
  publisher = {StatPearls Publishing},
  address = {Treasure Island (FL)},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK560699/},
  urldate = {2023-10-03},
  abstract = {All good research is based on a meticulous and well-designed question in the form of a hypothesis. To test this hypothesis, one must conduct an experiment with strict guidelines to obtain robust results. The results are then tested using statistics to examine its significance and conclude if a new treatment/ diagnostic modalities/biomarker is a better alternative to prevalent practice. Thus, statistical tests are an important component of research, especially in the fields of medicine.},
  copyright = {Copyright {\copyright} 2023, StatPearls Publishing LLC.},
  langid = {english},
  lccn = {NBK560699},
  pmid = {32809534},
  file = {/Users/nathanielyomogida/Zotero/storage/YMZ3C6VW/NBK560699.html}
}

@article{szumilasExplainingOddsRatios2010,
  title = {Explaining Odds Ratios},
  author = {Szumilas, Magdalena},
  year = {2010},
  month = aug,
  journal = {Journal of the Canadian Academy of Child and Adolescent Psychiatry = Journal De l'Academie Canadienne De Psychiatrie De L'enfant Et De L'adolescent},
  volume = {19},
  number = {3},
  pages = {227--229},
  issn = {2293-6122},
  langid = {english},
  pmcid = {PMC2938757},
  pmid = {20842279},
  keywords = {Odds Ratio,odds-ratio,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/7C532U9W/Szumilas (2010) Explaining odds ratios.pdf}
}

@article{tenhoveUpdatedGuidelinesSelecting2022,
  title = {Updated Guidelines on Selecting an Intraclass Correlation Coefficient for Interrater Reliability, with Applications to Incomplete Observational Designs},
  author = {Ten Hove, Debby and Jorgensen, Terrence D. and {van der Ark}, L. Andries},
  year = {2022},
  month = sep,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000516},
  abstract = {Several intraclass correlation coefficients (ICCs) are available to assess the interrater reliability (IRR) of observational measurements. Selecting an ICC is complicated, and existing guidelines have three major limitations. First, they do not discuss incomplete designs, in which raters partially vary across subjects. Second, they provide no coherent perspective on the error variance in an ICC, clouding the choice between the available coefficients. Third, the distinction between fixed or random raters is often misunderstood. Based on generalizability theory (GT), we provide updated guidelines on selecting an ICC for IRR, which are applicable to both complete and incomplete observational designs. We challenge conventional wisdom about ICCs for IRR by claiming that raters should seldom (if ever) be considered fixed. Also, we clarify how to interpret ICCs in the case of unbalanced and incomplete designs. We explain four choices a researcher needs to make when selecting an ICC for IRR, and guide researchers through these choices by means of a flowchart, which we apply to three empirical examples from clinical and developmental domains. In the Discussion, we provide guidance in reporting, interpreting, and estimating ICCs, and propose future directions for research into the ICCs for IRR. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english},
  pmid = {36048052},
  keywords = {To Read}
}

@incollection{tennyOddsRatio2024,
  title = {Odds {{Ratio}}},
  booktitle = {{{StatPearls}}},
  author = {Tenny, Steven and Hoffman, Mary R.},
  year = {2024},
  publisher = {StatPearls Publishing},
  address = {Treasure Island (FL)},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK431098/},
  urldate = {2024-01-27},
  abstract = {The odds ratio (OR)~is a~measure of how strongly an event is associated with exposure. The odds ratio is a ratio of two sets of odds: the odds of~the event~occurring~in an exposed group versus the odds of the event~occurring~in a non-exposed group.~ Odds ratios commonly~are~used to report case-control studies. The odds ratio helps identify how likely an exposure is to lead to a specific event. The larger the odds ratio, the higher odds that the event will occur with exposure. ~Odds ratios smaller than one imply the event has fewer odds of happening with the exposure.},
  copyright = {Copyright {\copyright} 2024, StatPearls Publishing LLC.},
  langid = {english},
  lccn = {NBK431098},
  pmid = {28613750},
  keywords = {Odds Ratio,To Read}
}

@article{thompsonAnalysisVarianceANOVA1999,
  title = {The {{Analysis}} of {{Variance}} ({{ANOVA}})},
  author = {Thompson, H. W. and Mera, R. and Prasad, C.},
  year = {1999},
  journal = {Nutritional Neuroscience},
  volume = {2},
  number = {1},
  pages = {43--55},
  issn = {1028-415X},
  doi = {10.1080/1028415X.1999.11747262},
  abstract = {This is the fourth in a series of articles devoted to a simplified description of experimental design, statistical analysis and interpretation. This article deals with a basic description of the analysis of variance (ANOVA) and its methods of computation and hypothesis testing. Examples are provided in which a one-way ANOVA is analyzed; methods for post-ANOVA pair-wise comparisons of treatment means are also discussed.},
  langid = {english},
  pmid = {27406694},
  keywords = {Analysis of variance,Experimental design,Multiple comparisons,Statistics,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/G6BNS8TW/thompson1999.pdf.pdf}
}

@article{zapfMeasuringInterraterReliability2016,
  title = {Measuring Inter-Rater Reliability for Nominal Data - Which Coefficients and Confidence Intervals Are Appropriate?},
  author = {Zapf, Antonia and Castell, Stefanie and Morawietz, Lars and Karch, Andr{\'e}},
  year = {2016},
  month = aug,
  journal = {BMC medical research methodology},
  volume = {16},
  pages = {93},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0200-9},
  abstract = {BACKGROUND: Reliability of measurements is a prerequisite of medical research. For nominal data, Fleiss' kappa (in the following labelled as Fleiss' K) and Krippendorff's alpha provide the highest flexibility of the available reliability measures with respect to number of raters and categories. Our aim was to investigate which measures and which confidence intervals provide the best statistical properties for the assessment of inter-rater reliability in different situations. METHODS: We performed a large simulation study to investigate the precision of the estimates for Fleiss' K and Krippendorff's alpha and to determine the empirical coverage probability of the corresponding confidence intervals (asymptotic for Fleiss' K and bootstrap for both measures). Furthermore, we compared measures and confidence intervals in a real world case study. RESULTS: Point estimates of Fleiss' K and Krippendorff's alpha did not differ from each other in all scenarios. In the case of missing data (completely at random), Krippendorff's alpha provided stable estimates, while the complete case analysis approach for Fleiss' K led to biased estimates. For shifted null hypotheses, the coverage probability of the asymptotic confidence interval for Fleiss' K was low, while the bootstrap confidence intervals for both measures provided a coverage probability close to the theoretical one. CONCLUSIONS: Fleiss' K and Krippendorff's alpha with bootstrap confidence intervals are equally suitable for the analysis of reliability of complete nominal data. The asymptotic confidence interval for Fleiss' K should not be used. In the case of missing data or data or higher than nominal order, Krippendorff's alpha is recommended. Together with this article, we provide an R-script for calculating Fleiss' K and Krippendorff's alpha and their corresponding bootstrap confidence intervals.},
  langid = {english},
  pmcid = {PMC4974794},
  pmid = {27495131},
  keywords = {Algorithms,Bootstrap,Breast Neoplasms,Confidence interval,Confidence Intervals,Data Interpretation Statistical,Female,Fleiss' K,Fleiss' kappa,Humans,Inter-rater heterogeneity,Krippendorff's alpha,Observer Variation,Reproducibility of Results,Retrospective Studies},
  file = {/Users/nathanielyomogida/Zotero/storage/I7LSNPXC/Zapf et al (2016) Measuring inter-rater reliability for nominal data - which coefficients and.pdf}
}
