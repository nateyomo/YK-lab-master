@article{bartkoIntraclassCorrelationCoefficient1966,
  title = {The {{Intraclass Correlation Coefficient}} as a {{Measure}} of {{Reliability}}},
  author = {Bartko, John J.},
  year = {1966},
  month = aug,
  journal = {Psychological Reports},
  volume = {19},
  number = {1},
  pages = {3--11},
  issn = {0033-2941, 1558-691X},
  doi = {10.2466/pr0.1966.19.1.3},
  url = {http://journals.sagepub.com/doi/10.2466/pr0.1966.19.1.3},
  urldate = {2024-02-02},
  abstract = {A procedure for estimating the reliability of sets of ratings in terms of the intraclass correlation coefficient is discussed. The procedure is based upon the analysis of variance and the estimation of variance components. For the one-way classification the intraclass correlation coefficient defined as the ratio of variances can be interpreted as a correlation coefficient. Caution, however, is urged in the application of the definition to a two-way model, i.e., one in which between-rater variance is removed. It is maintained that the frequent use of the standard definition of the one-way intraclass correlation coefficient applied to the two-way classification is not a proper procedure if in fact the coefficient is to be interpreted as a correlation coefficient. Definitions for reliability obtained from the two-way models are given which can legitimately be considered correlation coefficients.},
  langid = {english},
  file = {/Users/nathanielyomogida/Zotero/storage/SDCZ7AZ6/Bartko (1966) The Intraclass Correlation Coefficient as a Measure of Reliability.pdf}
}

@article{birkimerBackBasicsPercentage1979,
  title = {Back to Basics: {{Percentage}} Agreement Measures Are Adequate, but There Are Easier Ways},
  shorttitle = {Back to Basics},
  author = {Birkimer, J. C. and Brown, J. H.},
  year = {1979},
  journal = {Journal of Applied Behavior Analysis},
  volume = {12},
  number = {4},
  pages = {535--543},
  issn = {0021-8855},
  doi = {10.1901/jaba.1979.12-535},
  abstract = {Percentage agreement measures of interobserver agreement or "reliability" have traditionally been used to summarize observer agreement from studies using interval recording, time-sampling, and trial-scoring data collection procedures. Recent articles disagree on whether to continue using these percentage agreement measures, and on which ones to use, and what to do about chance agreements if their use is continued. Much of the disagreement derives from the need to be reasonably certain we do not accept as evidence of true interobserver agreement those agreement levels which are substantially probable as a result of chance observer agreement. The various percentage agreement measures are shown to be adequate to this task, but easier ways are discussed. Tables are given to permit checking to see if obtained disagreements are unlikely due to chance. Particularly important is the discovery of a simple rule that, when met, makes the tables unnecessary. If reliability checks using 50 or more observation occasions produce 10\% or fewer disagreements, for behavior rates from 10\% through 90\%, the agreement achieved is quite improbably the result of chance agreement.},
  langid = {english},
  pmcid = {PMC1311476},
  pmid = {16795610},
  file = {/Users/nathanielyomogida/Zotero/storage/XJSY664R/Birkimer_Brown (1979) Back to basics.pdf}
}

@article{buntingPracticalGuideAssess2019,
  title = {A {{Practical Guide}} to {{Assess}} the {{Reproducibility}} of {{Echocardiographic Measurements}}},
  author = {Bunting, Karina V. and Steeds, Richard P. and Slater, Karin and Rogers, Jennifer K. and Gkoutos, Georgios V. and Kotecha, Dipak},
  year = {2019},
  month = dec,
  journal = {Journal of the American Society of Echocardiography},
  volume = {32},
  number = {12},
  pages = {1505--1515},
  issn = {08947317},
  doi = {10.1016/j.echo.2019.08.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0894731719309460},
  urldate = {2024-03-04},
  langid = {english},
  keywords = {To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/ARH6F2IT/Bunting et al (2019) A Practical Guide to Assess the Reproducibility of Echocardiographic.pdf}
}

@article{gisevInterraterAgreementInterrater2013,
  title = {Interrater Agreement and Interrater Reliability: Key Concepts, Approaches, and Applications},
  shorttitle = {Interrater Agreement and Interrater Reliability},
  author = {Gisev, Natasa and Bell, J. Simon and Chen, Timothy F.},
  year = {2013},
  journal = {Research in social \& administrative pharmacy: RSAP},
  volume = {9},
  number = {3},
  pages = {330--338},
  issn = {1934-8150},
  doi = {10.1016/j.sapharm.2012.04.004},
  abstract = {Evaluations of interrater agreement and interrater reliability can be applied to a number of different contexts and are frequently encountered in social and administrative pharmacy research. The objectives of this study were to highlight key differences between interrater agreement and interrater reliability; describe the key concepts and approaches to evaluating interrater agreement and interrater reliability; and provide examples of their applications to research in the field of social and administrative pharmacy. This is a descriptive review of interrater agreement and interrater reliability indices. It outlines the practical applications and interpretation of these indices in social and administrative pharmacy research. Interrater agreement indices assess the extent to which the responses of 2 or more independent raters are concordant. Interrater reliability indices assess the extent to which raters consistently distinguish between different responses. A number of indices exist, and some common examples include Kappa, the Kendall coefficient of concordance, Bland-Altman plots, and the intraclass correlation coefficient. Guidance on the selection of an appropriate index is provided. In conclusion, selection of an appropriate index to evaluate interrater agreement or interrater reliability is dependent on a number of factors including the context in which the study is being undertaken, the type of variable under consideration, and the number of raters making assessments.},
  langid = {english},
  pmid = {22695215},
  keywords = {Data Interpretation Statistical,Finish reading,Humans,Observer Variation,Pharmacy Administration,Reproducibility of Results,Research Design},
  file = {/Users/nathanielyomogida/Zotero/storage/EDZEQ7C6/Gisev et al (2013) Interrater agreement and interrater reliability.pdf}
}

@article{kooGuidelineSelectingReporting2016,
  title = {A {{Guideline}} of {{Selecting}} and {{Reporting Intraclass Correlation Coefficients}} for {{Reliability Research}}},
  author = {Koo, Terry K. and Li, Mae Y.},
  year = {2016},
  month = jun,
  journal = {Journal of Chiropractic Medicine},
  volume = {15},
  number = {2},
  pages = {155--163},
  issn = {1556-3707},
  doi = {10.1016/j.jcm.2016.02.012},
  abstract = {OBJECTIVE: Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis. DISCUSSION FOR RESEARCHERS: There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, "model," "type," and "definition" selections. DISCUSSION FOR READERS: When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95\% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively. CONCLUSION: This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.},
  langid = {english},
  pmcid = {PMC4913118},
  pmid = {27330520},
  keywords = {Finish reading,Reliability and validity,Research,Statistics},
  file = {/Users/nathanielyomogida/Zotero/storage/43PHZTQP/Koo_Li (2016) A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for.pdf}
}

@article{liljequistIntraclassCorrelationDiscussion2019,
  title = {Intraclass Correlation - {{A}} Discussion and Demonstration of Basic Features},
  author = {Liljequist, David and Elfving, Britt and Skavberg Roaldsen, Kirsti},
  year = {2019},
  journal = {PloS One},
  volume = {14},
  number = {7},
  pages = {e0219854},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0219854},
  abstract = {A re-analysis of intraclass correlation (ICC) theory is presented together with Monte Carlo simulations of ICC probability distributions. A partly revised and simplified theory of the single-score ICC is obtained, together with an alternative and simple recipe for its use in reliability studies. Our main, practical conclusion is that in the analysis of a reliability study it is neither necessary nor convenient to start from an initial choice of a specified statistical model. Rather, one may impartially use all three single-score ICC formulas. A near equality of the three ICC values indicates the absence of bias (systematic error), in which case the classical (one-way random) ICC may be used. A consistency ICC larger than absolute agreement ICC indicates the presence of non-negligible bias; if so, classical ICC is invalid and misleading. An F-test may be used to confirm whether biases are present. From the resulting model (without or with bias) variances and confidence intervals may then be calculated. In presence of bias, both absolute agreement ICC and consistency ICC should be reported, since they give different and complementary information about the reliability of the method. A clinical example with data from the literature is given.},
  langid = {english},
  pmcid = {PMC6645485},
  pmid = {31329615},
  keywords = {Correlation of Data,Data Interpretation Statistical,Electromyography,Humans,Monte Carlo Method,Software,To Read},
  file = {/Users/nathanielyomogida/Zotero/storage/24YTZRK9/Liljequist et al. - 2019 - Intraclass correlation - A discussion and demonstr.pdf}
}

@article{mchughInterraterReliabilityKappa2012,
  title = {Interrater Reliability: The Kappa Statistic},
  shorttitle = {Interrater Reliability},
  author = {McHugh, Mary L.},
  year = {2012},
  journal = {Biochemia Medica},
  volume = {22},
  number = {3},
  pages = {276--282},
  issn = {1330-0962},
  abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
  langid = {english},
  pmcid = {PMC3900052},
  pmid = {23092060},
  keywords = {Data Interpretation Statistical,Observer Variation,Reproducibility of Results,To Read}
}

@article{streinerReliabilityStatisticsCommentarySeries2016a,
  title = {Statistics {{Commentary Series}}: {{Commentary}} \#15-{{Reliability}}},
  shorttitle = {Statistics {{Commentary Series}}},
  author = {Streiner, David L.},
  year = {2016},
  month = aug,
  journal = {Journal of Clinical Psychopharmacology},
  volume = {36},
  number = {4},
  pages = {305--307},
  issn = {1533-712X},
  doi = {10.1097/JCP.0000000000000517},
  langid = {english},
  pmid = {27219092},
  keywords = {Completed,Humans,Psychometrics,Reproducibility of Results,Statistics as Topic},
  file = {/Users/nathanielyomogida/Zotero/storage/9JX4IH73/Streiner (2016) Statistics Commentary Series.pdf}
}

@article{tenhoveUpdatedGuidelinesSelecting2022,
  title = {Updated Guidelines on Selecting an Intraclass Correlation Coefficient for Interrater Reliability, with Applications to Incomplete Observational Designs},
  author = {Ten Hove, Debby and Jorgensen, Terrence D. and {van der Ark}, L. Andries},
  year = {2022},
  month = sep,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000516},
  abstract = {Several intraclass correlation coefficients (ICCs) are available to assess the interrater reliability (IRR) of observational measurements. Selecting an ICC is complicated, and existing guidelines have three major limitations. First, they do not discuss incomplete designs, in which raters partially vary across subjects. Second, they provide no coherent perspective on the error variance in an ICC, clouding the choice between the available coefficients. Third, the distinction between fixed or random raters is often misunderstood. Based on generalizability theory (GT), we provide updated guidelines on selecting an ICC for IRR, which are applicable to both complete and incomplete observational designs. We challenge conventional wisdom about ICCs for IRR by claiming that raters should seldom (if ever) be considered fixed. Also, we clarify how to interpret ICCs in the case of unbalanced and incomplete designs. We explain four choices a researcher needs to make when selecting an ICC for IRR, and guide researchers through these choices by means of a flowchart, which we apply to three empirical examples from clinical and developmental domains. In the Discussion, we provide guidance in reporting, interpreting, and estimating ICCs, and propose future directions for research into the ICCs for IRR. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english},
  pmid = {36048052},
  keywords = {To Read}
}

@article{zapfMeasuringInterraterReliability2016,
  title = {Measuring Inter-Rater Reliability for Nominal Data - Which Coefficients and Confidence Intervals Are Appropriate?},
  author = {Zapf, Antonia and Castell, Stefanie and Morawietz, Lars and Karch, Andr{\'e}},
  year = {2016},
  month = aug,
  journal = {BMC medical research methodology},
  volume = {16},
  pages = {93},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0200-9},
  abstract = {BACKGROUND: Reliability of measurements is a prerequisite of medical research. For nominal data, Fleiss' kappa (in the following labelled as Fleiss' K) and Krippendorff's alpha provide the highest flexibility of the available reliability measures with respect to number of raters and categories. Our aim was to investigate which measures and which confidence intervals provide the best statistical properties for the assessment of inter-rater reliability in different situations. METHODS: We performed a large simulation study to investigate the precision of the estimates for Fleiss' K and Krippendorff's alpha and to determine the empirical coverage probability of the corresponding confidence intervals (asymptotic for Fleiss' K and bootstrap for both measures). Furthermore, we compared measures and confidence intervals in a real world case study. RESULTS: Point estimates of Fleiss' K and Krippendorff's alpha did not differ from each other in all scenarios. In the case of missing data (completely at random), Krippendorff's alpha provided stable estimates, while the complete case analysis approach for Fleiss' K led to biased estimates. For shifted null hypotheses, the coverage probability of the asymptotic confidence interval for Fleiss' K was low, while the bootstrap confidence intervals for both measures provided a coverage probability close to the theoretical one. CONCLUSIONS: Fleiss' K and Krippendorff's alpha with bootstrap confidence intervals are equally suitable for the analysis of reliability of complete nominal data. The asymptotic confidence interval for Fleiss' K should not be used. In the case of missing data or data or higher than nominal order, Krippendorff's alpha is recommended. Together with this article, we provide an R-script for calculating Fleiss' K and Krippendorff's alpha and their corresponding bootstrap confidence intervals.},
  langid = {english},
  pmcid = {PMC4974794},
  pmid = {27495131},
  keywords = {Algorithms,Bootstrap,Breast Neoplasms,Confidence interval,Confidence Intervals,Data Interpretation Statistical,Female,Fleiss' K,Fleiss' kappa,Humans,Inter-rater heterogeneity,Krippendorff's alpha,Observer Variation,Reproducibility of Results,Retrospective Studies},
  file = {/Users/nathanielyomogida/Zotero/storage/I7LSNPXC/Zapf et al (2016) Measuring inter-rater reliability for nominal data - which coefficients and.pdf}
}
